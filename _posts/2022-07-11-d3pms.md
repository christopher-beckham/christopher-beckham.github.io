---
title: My notes on discrete denoising diffusion models (D3PMs)
layout: default_latex
---


# Table of Contents

1.  [Denoising discrete diffusion probabilistic models (D3PM)](#orgc0b2bb2)
    1.  [Characterisation of the reverse process](#orgef42be2)
    2.  [The t-step conditional for the forward process](#orgd1f3f75)
2.  [References](#orgc09633a)

$$\newcommand{\xx}{\boldsymbol{x}}$$ $$\newcommand{\pt}{p_{\theta}}$$ $$\newcommand{\QQ}{\boldsymbol{Q}}$$

<a id="orgc0b2bb2"></a>

# Denoising discrete diffusion probabilistic models (D3PMs)

Here are some of my thoughts on a (semi-)recent paper that came out by [Austin et al.](https://proceedings.neurips.cc/paper/2021/hash/958c530554f78bcd8e97125b70e6973d-Abstract.html) It proposes a variant of the (continuous) diffusion model for discrete data, which is typically an awkward modality to deal with since almost everything we do with deep neural networks is in continuous space. Many types of data can be represented as discrete, for instance text, molecules, graphs, and even images if we don't dequantise their pixel values (which are typically ordinal variables taking on values (0-255)). Representing data as discrete variables can also be a reasonable form of compression.

In this short blog post I won't be going into the details of the paper, but I will be mainly presenting some math I did to help me better understand the proposed algorithm.

<div id="images">
<br />
<figure>
<img class="figg" src="/assets/03/d3pms.png" alt="" />
</figure>
<figcaption>Figure 1: overview of D3PMs. Source: original paper [1]</figcaption>
<br />
</div>

<a id="orgef42be2"></a>

## Characterisation of the reverse process

Probably the most confusing aspect of the paper for me was Section 3.3, which explains how the revese process is parameterised. 
Instead of having the neural network (reverse process) directly predict $$\xx_{t-1}$$ directly from $$\xx_{t}$$, some weird (but supposedly justified) design decision is made which partly involves predicting $$p(\xx_0 | \xx_t)$$ instead.

Writing this equation out here. I will be using superscript $$(j)$$ to index into the $$j$$'th value of $$\xx_0$$, since it is a probability distribution over $$k$$ latent variables.

$$\begin{align}
\pt(\xx_{t-1}|\xx_{t}) &= \frac{\sum_{j} \pt(\xx_{t-1}, \xx_{t}, \xx_{0}^{(j)})}{\pt(\xx_t)} \tag{1} \\
&= \frac{\sum_{j} \pt(\xx_{t-1}, \xx_{t} | \xx_{0}^{(j)}) \pt(\xx_0^{(j)} | \xx_t) \pt(\xx_t)  }{\pt(\xx_t)} \tag{2} \\
&= \sum_{j} \pt(\xx_{t-1}, \xx_{t} | \xx_{0}^{(j)}) \pt(\xx_0^{(j)} | \xx_t) \tag{3} \\
&\approx \sum_{j} q(\xx_{t-1}, \xx_{t} | \xx_{0}^{(j)}) \pt(\xx_0^{(j)} | \xx_t) \tag{4} \\
&= \sum_{j} q(\xx_{t-1} | \xx_{0}^{(j)}) q(\xx_t | \xx_{t-1}, \xx_0^{(j)}) \pt(\xx_0^{(j)} | \xx_t) \tag{5} \\
&= \sum_{j} q(\xx_{t-1} | \xx_{0}^{(j)}) q(\xx_t | \xx_{t-1}) \pt(\xx_0^{(j)} | \xx_t) \tag{6} \\
&= q(\xx_t | \xx_{t-1}) \sum_{j} q(\xx_{t-1} | \xx_{0}^{(j)}) \pt(\xx_0^{(j)} | \xx_t) \tag{7}
\end{align}$$

The approximation comes from the fact that in line (3) we are using $$\pt$$ for the forward (noising) process to go from $$\xx_{t-1}$$ and $$\xx_t$$ from $$\xx_0$$, but this is actually what $$q$$ is for! ($$\pt$$ is meant to perform the reverse process, which is denoising.) Because of this, we should just change this term to use $$q$$, hence the switch to an approximate equals $$\approx$$ sign in line (4). For the remaining lines:

-   Line (5): due to conditional independence,
$$q(\xx_{t-1}, \xx_{t}|\xx_{0})$$ simplifies into $$q(\xx_{t-1}|\xx_{0}) \cdot q(\xx_t | \xx_{t-1}, \xx_{0})$$.
-   Line (6): due to the Markov property, it is redundant to condition on $$\xx_0$$ 
for the latter term, so we can write $$q(\xx_t|\xx_{t-1},\xx_0)$$ as $$q(\xx_t|\xx_{t-1})$$.
-   Line (7): $$q(\xx_t | \xx_{t-1})$$ 
can be moved outside the summation since there is no dependence on $$\xx_0$$.

This particular equation is implemented [here](https://github.com/google-research/google-research/blob/master/d3pm/images/diffusion_categorical.py#L399-L424). When this fn is called with:

-   `fact2 = self._at_onehot(self.q_mats, t-1, jax.nn.softmax(x_start, axis=-1)` basically implements the summation term in line (7).
-   The first term of line (7) is implemented as: `fact1 = self._at(self.transpose_q_onestep_mats, t, x_t)`.
-   Note that this is computed in logit space, i.e. `log(equation 7) = log(fact1) + log(fact2)`.

<a id="orgd1f3f75"></a>

## The t-step conditional for the forward process

One thing that initially confused me was how the t-step conditional was implemented in the official [code](https://github.com/google-research/google-research/tree/master/d3pm). This is what it looks like mathematically (Equation (3) in their paper):

$$\begin{align}
q(\xx_{t-1}|\xx_t, \xx_0) = \frac{q(\xx_t | \xx_{t-1}, \xx_0) q(\xx_{t-1}|\xx_0) }{q(\xx_t | \xx_0)} = \text{Cat}\Big( \xx_{t-1}; \boldsymbol{p} = \frac{\xx_t \QQ_t^{T} \cdot \xx_0 \bar{\QQ}_{t-1}}{\xx_0 \bar{\QQ}_t \xx_{t}^{T}} \Big)
\end{align}$$

In code the implementation can be found in [q_posterior_logits](https://github.com/google-research/google-research/blob/master/d3pm/images/diffusion_categorical.py#L399-L424) (when the function argument `x_start_logits=False`). We can see that the following term implements the first term in the numerator $$\xx_t \QQ_t^{T}$$:

`fact1 = self._at(self.transpose_q_onestep_mats, t, x_t)` 

The second term in the numerator ($$\xx_0 \bar{\QQ}_{t-1}$$)is simply:

`fact2 = self._at(self.q_mats, t-1, x_start)`.

My confusion initially lied with the fact that the denominator was missing, until Jacob Austin (the paper author) very nicely pointed out to me that implementing just the numerator is implementing just the logits of said distribution, i.e. the denominator is what normalises it so that $$p$$ sums to one. Logits are used because the subsequently used method [categorical_kl_logits](https://github.com/google-research/google-research/blob/e3d00617cb28064b6e96ab4e2485079f0ca5a763/d3pm/images/utils.py#L72-L89) takes logits as arguments instead of probability distributions.


# References

<a id="orgc09633a"></a>

- {: #ref_d3pm } \[1\]: Austin, J., Johnson, D. D., Ho, J., Tarlow, D., & van den Berg, R. (2021). Structured denoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems, 34, 17981-17993.