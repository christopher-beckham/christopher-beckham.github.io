#+OPTIONS: toc:nil
#+LATEX_HEADER: \newcommand{\xx}{\boldsymbol{x}}
#+LATEX_HEADER: \newcommand{\xxtilde}{\tilde{\boldsymbol{x}}}
#+LATEX_HEADER: \newcommand{\psigma}{p_{\sigma_i}}
#+LATEX_HEADER: \newcommand{\st}{s_{\theta}}

#+BEGIN_EXPORT html
---
title: Derivations for score-based generative models
layout: default_latex
---

<h1>Derivations for score-based generative models</h1>

<div hidden>
<!-- This should be consistent with LATEX_HEADER -->
$$\newcommand{\xx}{\boldsymbol{x}}$$
$$\newcommand{\xxtilde}{\tilde{\boldsymbol{x}}}$$
$$\newcommand{\psigma}{p_{\sigma_i}}$$
$$\newcommand{\st}{s_{\theta}}$$
</div>
#+END_EXPORT

Without any fancy intros, I'm just going to jump straight into it. The score matching loss is defined as follows =song2020improved=:

\begin{align}
\text{loss} = \mathbb{E}_{\xx \sim p(\xx)}\mathbb{E}_{i \sim \pi(i)} \mathbb{E}_{\xxtilde \sim \psigma(\xxtilde|\xx)}\ \Big\| \sigma_i \st(\xxtilde, \sigma_i) + \frac{\xxtilde - \xx}{\sigma_i} \Big\|^{2}_{2},
\end{align} 
where we denote $\pi$ as some discrete distribution over the number of noise scales $i \in \{1, \dots, L\}$. For example, if we sample these indices uniformly then $\pi(i) = \text{Uniform}(i; 1, L)$.

* Reparameterisation trick

Thanks to the re-parameterisation trick, $\xxtilde \sim \mathcal{N}(\xxtilde; \xx, \sigma)$ can expressed as first sampling standard Gaussian noise $\epsilon \sim \mathcal{N}(0,1)$ and then computing $\xxtilde = \xx + \epsilon\sigma$. Therefore, the above equation can be expressed and simplified to the following:

\begin{align}
& \mathbb{E}_{\xx \sim p(\xx)}\mathbb{E}_{i \sim \pi(i)}\mathbb{E}_{\epsilon \sim \mathcal{N}(0,I)} \ \Big\| \sigma_i \st(\xx + \epsilon \sigma_i, \sigma_i) + \frac{\xx + \epsilon\sigma_i - \xx}{\sigma_i} \Big\|^{2}_{2} \\
& = \mathbb{E}_{\xx \sim p(\xx)}\mathbb{E}_{i \sim \pi(i)}\mathbb{E}_{\epsilon \sim \mathcal{N}(0,I)} \ \Big\| \sigma_i \st(\xx + \epsilon \sigma_i, \sigma_i) + \epsilon \Big\|^{2}_{2}. \tag{1}
\end{align} 

* Simplifying noise conditioning

In =song2020improved=, one of the tricks that is proposed is to do away with fancy conditioning techniques inside the U-Net for $\sigma_i$ as was done in =song2019generative=. Instead, one just simply scales the output of the U-Net by $\sigma_i$ instead, which gives us $\st(\xxtilde, \sigma_i) = \st(\xxtilde) / \sigma_i$. Therefore, we can simplify the above equation even further:
\begin{align}
& \mathbb{E}_{\xx \sim p(\xx)}\mathbb{E}_{i \sim \pi(i)}\mathbb{E}_{\epsilon \sim \mathcal{N}(0,I)} \ \Big\| \sigma_i \st(\xx + \epsilon \sigma_i, \sigma_i) + \epsilon \Big\|^{2}_{2} \\
& = \mathbb{E}_{\xx \sim p(\xx)}\mathbb{E}_{i \sim \pi(i)}\mathbb{E}_{\epsilon \sim \mathcal{N}(0,I)} \ \Big\| \sigma_i \st(\xx + \epsilon \sigma_i) / \sigma_i + \epsilon \Big\|^{2}_{2} \\
& = \mathbb{E}_{\xx \sim p(\xx)}\mathbb{E}_{i \sim \pi(i)}\mathbb{E}_{\epsilon \sim \mathcal{N}(0,I)} \ \Big\| \st(\xx + \epsilon \sigma_i) + \epsilon \Big\|^{2}_{2}. \tag{2}
\end{align}

* Understanding the implementation in official code

The official implementation of the loss doesn't look entirely clear at first, but we can do some derivations to show that it is equivalent to equation (1) if we do some algebra.

The official code is [[https://github.com/ermongroup/ncsnv2/blob/master/losses/dsm.py][here,]] and the score matching loss is implemented as follows:

#+BEGIN_SRC python
# def anneal_dsm_score_estimation(scorenet, samples, sigmas, labels=None, anneal_power=2., hook=None):
# ...
perturbed_samples = samples + noise
target = - 1 / (used_sigmas ** 2) * noise
scores = scorenet(perturbed_samples, labels)
target = target.view(target.shape[0], -1)
scores = scores.view(scores.shape[0], -1)
loss = 1 / 2. * ((scores - target) ** 2).sum(dim=-1) * used_sigmas.squeeze() ** anneal_power
# ...
#+END_SRC

When =anneal_power=2= (the default), then $\lambda(\sigma_i) = \sigma_i^2$ (see the original paper for what $\lambda$ is). For a given triplet $(\xx, \sigma_i, \epsilon)$, the loss is:

\begin{align}
\text{loss}_{\xx, \sigma_i, \epsilon} & = \sigma_i^2 \frac{1}{2}\Big\| \st(\xx + \epsilon\sigma_i, i) - (\frac{-1}{\sigma_i^2} \epsilon\sigma_i) \Big\|^{2}_2 \\
& = \sigma_i^2 \frac{1}{2}\Big\| \st(\xx + \epsilon\sigma_i, i) + \frac{\epsilon}{\sigma_i} \Big\|^{2}_{2} \ \ \text{(simplify)}\\
& = \sigma_i^2 \frac{1}{2} \sum_{j} \Big[ \st(\xx + \epsilon\sigma_i, i)^2 + \frac{2\epsilon}{\sigma_i} \st(\xx+\epsilon\sigma, i) + \frac{\epsilon^2}{\sigma_i^2}\Big]_{j} \ \ \text{(expand quadratic)} \\
& = \frac{1}{2} \sum_{j} \Big[ \sigma_i^2 \st(\xx + \epsilon\sigma_i, i)^2 + 2\epsilon\sigma_i \st(\xx+\epsilon\sigma, i) + \epsilon^2\Big]_{j} \ \ \text{(distribute $\sigma_i$)} \\
& = \frac{1}{2}\Big\| \sigma_i \st(\xx+\epsilon\sigma_i, i) + \epsilon \Big\|^{2}_{2}. \ \ \text{(re-factorise quadratic)}
\end{align}

...and we're done! Of course, if you want, you can use the noise conditioning simplification to obtain equation (2) again.

* References

- =song2020improved= Song, Y., & Ermon, S. (2020). Improved techniques for training score-based generative models. Advances in neural information processing systems, 33(), 12438â€“12448.
- =song2019generative= Song, Y., & Ermon, S. (2019). Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32(), .
