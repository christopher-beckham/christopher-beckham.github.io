#+OPTIONS: toc:nil
#+LATEX_HEADER: \definecolor{purple}{RGB}{122, 24, 128}
#+LATEX_HEADER: \newcommand{\xx}{\bm{x}}
#+LATEX_HEADER: \newcommand{\zz}{\bm{z}}
#+LATEX_HEADER: \newcommand{\yy}{\bm{y}}
#+LATEX_HEADER: \newcommand{\XX}{\bm{X}}
#+LATEX_HEADER: \newcommand{\ZZ}{\bm{Z}}
#+LATEX_HEADER: \newcommand{\YY}{\bm{Y}}
#+LATEX_HEADER: \newcommand{\xxt}{\tilde{\xx}}
#+LATEX_HEADER: \newcommand{\yt}{\tilde{y}}
#+LATEX_HEADER: \newcommand{\pt}{\textcolor{green}{p_{\theta}}}
#+LATEX_HEADER: \newcommand{\ft}{f_{\theta}}
#+LATEX_HEADER: \newcommand{\argmax}{\text{argmax}}
#+LATEX_HEADER: \newcommand{\Dtrain}{\mathcal{D}_{\text{train}}}
#+LATEX_HEADER: \newcommand{\Dvalid}{\mathcal{D}_{\text{val}}}
#+LATEX_HEADER: \newcommand{\circleone}{\textcircled{\small{1}}}
#+LATEX_HEADER: \newcommand{\circletwo}{\textcircled{\small{2}}}
#+LATEX_HEADER: \newcommand{\circlethree}{\textcircled{\small{3}}}
#+LATEX_HEADER: \newcommand{\circlefour}{\textcircled{\small{4}}}
#+LATEX_HEADER: \newcommand{\pzgivenx}{\textcolor{green}{p_{\theta}}(\zz|\xx)}
#+LATEX_HEADER: \newcommand{\pxgivenz}{\textcolor{green}{p_{\theta}}(\xx|\zz)}
#+LATEX_HEADER: \newcommand{\qzgivenx}{\textcolor{purple}{q_{\phi}}(\zz|\xx)}
#+LATEX_HEADER: \newcommand{\qzgivenxi}{\textcolor{purple}{q_{\phi}}(\zz|\zz^{(i)})}
#+LATEX_HEADER: \newcommand{\qx}{\textcolor{purple}{q}(\xx)}
#+LATEX_HEADER: \newcommand{\qp}{\textcolor{purple}{q_{\phi}}}
#+LATEX_HEADER: \newcommand{\qpink}{\textcolor{purple}{q}}
#+LATEX_HEADER: \newcommand{\pgreen}{\textcolor{green}{p}}
#+LATEX_HEADER: \newcommand{\ptgreen}{\textcolor{green}{p_{\theta}}}
#+LATEX_HEADER: \newcommand{\ptpgreen}{\textcolor{green}{p_{\theta, \psi}}}
#+LATEX_HEADER: \newcommand{\qpz}{\textcolor{purple}{q_{\phi}(\zz)}}
#+LATEX_HEADER: \newcommand{\pz}{\textcolor{green}{p}(\zz)}
#+LATEX_HEADER: \newcommand{\pzx}{\textcolor{green}{p_{\theta}}(\zz, \xx)}
#+LATEX_HEADER: \newcommand{\qz}{\textcolor{purple}{q}(\zz)}
#+LATEX_HEADER: \newcommand{\qzx}{\textcolor{purple}{q}(\zz, \xx)}
#+LATEX_HEADER: \newcommand{\phip}{\color{purple}{\phi}}
#+LATEX_HEADER: \newcommand{\thetagr}{\color{green}{\theta}}
#+LATEX_HEADER: \newcommand{\kldiv}{ \mathcal{D}_{\text{KL}} }
#+LATEX_HEADER: \newcommand{\elbo}{ \text{ELBO}(\textcolor{purple}{\phi}, \textcolor{green}{\theta}) }
#+LATEX_HEADER: \newcommand{\myeq}[1]{\stackrel{\mathclap{\normalfont\mbox{#1}}}{=}}


#+BEGIN_EXPORT html
---
title: Trials and tribulations of conditional variational autoencoders
description: In this blog post we discuss difficulties in correctly training conditional variational autoencoders. We reason about the evidence lower bound through the lens of mutual information, explain the 'ELBO dilemma', and illustrate it through simple toy experiments on a 2D Gaussian mixture dataset.
layout: default_latex
---

<h1>Trials and tributions of conditional variational autoencoders</h1>

<div hidden>
<!-- 
Differences to Latex header:
- Replace \bm with \boldsymbol
- Do not use textcolor here it doesn't work, have to use color  since mathjax likes that instead
- Circles have to be replaced with (1), ... (4)
-->
$$\newcommand{\xx}{\boldsymbol{x}}$$
$$\newcommand{\zz}{\boldsymbol{z}}$$
$$\newcommand{\yy}{\boldsymbol{y}}$$
$$\newcommand{\XX}{\boldsymbol{X}}$$
$$\newcommand{\ZZ}{\boldsymbol{Z}}$$
$$\newcommand{\YY}{\boldsymbol{Y}}$$
$$\newcommand{\xxt}{\tilde{\boldsymbol{x}}}$$
$$\newcommand{\yt}{\tilde{y}}$$
$$\newcommand{\pt}{\color{green}{p_{\theta}}}$$
$$\newcommand{\pto}{p_{\theta, \omega}}$$
$$\newcommand{\ft}{f_{\theta}}$$
$$\newcommand{\argmax}{\text{argmax}}$$
$$\newcommand{\Dtrain}{\mathcal{D}_{\text{train}}}$$
$$\newcommand{\Dvalid}{\mathcal{D}_{\text{val}}}$$
$$\newcommand{\circleone}{(a)}$$
$$\newcommand{\circletwo}{(b)}$$
$$\newcommand{\circlethree}{(c)}$$
$$\newcommand{\circlefour}{(d)}$$
$$\newcommand{\pzgivenx}{\color{green}{p_{\theta}}(\zz|\xx)}$$
$$\newcommand{\pxgivenz}{\color{green}{p_{\theta}}(\xx|\zz)}$$
$$\newcommand{\qzgivenx}{\color{purple}{q_{\phi}}(\zz|\xx)}$$
$$\newcommand{\qzgivenxi}{\color{purple}{q_{\phi}}(\zz|\zz^{(i)})}$$
$$\newcommand{\qx}{\color{purple}{q}(\xx)}$$
$$\newcommand{\qp}{\color{purple}{q_{\phi}}}$$
$$\newcommand{\qpink}{\color{purple}{q}}$$
$$\newcommand{\pgreen}{\color{green}{p}}$$
$$\newcommand{\ptgreen}{\color{green}{p_{\theta}}}$$
$$\newcommand{\ptpgreen}{\color{green}{p_{\theta, \psi}}}$$
$$\newcommand{\phip}{\color{purple}{\phi}}$$
$$\newcommand{\thetagr}{\color{green}{\theta}}$$
$$\newcommand{\qpz}{\color{purple}{q_{\phi}(\zz)}}$$
$$\newcommand{\pz}{\color{green}{p}(\zz)}$$
$$\newcommand{\pzx}{\color{green}{p_{\theta}}(\zz, \xx)}$$
$$\newcommand{\qz}{\color{purple}{q}(\zz)}$$
$$\newcommand{\qzx}{\color{purple}{q}(\zz, \xx)}$$
$$\newcommand{\kldiv}{ \mathcal{D}_{\text{KL}} }$$
$$\newcommand{\elbo}{ \text{ELBO}(\color{purple}{\phi}, \color{green}{\theta}) }$$
$$\newcommand{\myeq}[1]{\overset{#1}{=}}$$
</div>

#+END_EXPORT

#+BEGIN_COMMENT
Use LatexIt to generate.

Preamble:

\usepackage{tikz}

--------------

Dependent C-VAE:

\begin{tikzpicture}
    \node[shape=circle,draw=black] (Y) at (0,0) {Y};
    \node[shape=circle,draw=black] (Z) at (2,0) {Z};
    \node[shape=circle,draw=black] (X) at (4,0) {X};
    \path [->](Y) edge node[left] {} (Z);
    \path [->](Z) edge node[left] {} (X);
    \path [->](Y) edge[bend right] node[left] {} (X);
\end{tikzpicture}

Independent C-VAE:

\begin{tikzpicture}
    \node[shape=circle,draw=black] (Y) at (0,0.5) {Y};
    \node[shape=circle,draw=black] (Z) at (4,0.5) {Z};
    \node[shape=circle,draw=black] (X) at (2,0) {X};
    \path [->](Y) edge node[left] {} (X);
    \path [->](Z) edge node[left] {} (X);
\end{tikzpicture}
#+END_COMMENT

#+TOC: headlines 2

* Introduction

This is yet another excerpt from my upcoming PhD thesis. I actually wanted to write this several years ago after some really painful experiences I had with getting conditional VAEs to work on a generative modelling project I was working on. To the best of my knowledge, I haven't seen any paper that talks in depth about these difficulties and so I am quite happy to finally share them with everyone. While there are countless tutorials and blog posts out there on (conditional) VAEs, this one is /mine/, and is a culmination of many years of experience I've had coding up, wrangling with, and dealing with them. We will be going deep into the rabbit hole with this one, guys.

# context: issue is y being ignored
# content: can be resolved but at a cost to sample quality
# conclusion: i will demontrate this theoretically and empirically
In this post I want to shed light on a particular issue that that occurs when training a /conditional/ VAE (a cVAE), and that is the network ignoring the latent variable $\yy$. Generally speaking, when we train a generative model that is being conditioned on /more than one latent variable/ (i.e. a $\zz$ and $\yy$), the model can end up ignoring one of the variables. For cVAEs, this can be addressed but it comes at a significant cost to sample quality. In this post I will illustrate why this is the case through an alternative formulation of the ELBO, and I will also demonstrate this empirically on a simple 2D dataset where the conditioning variable $\yy$ denotes which Gaussian cluster $\xx$ belongs to.

I will save the basics for unconditional VAEs for another blog post. I should be publishing part one first, but I've really wanted to publish this part for a long time  -- in fact I wanted to do it roughly two years ago but kept putting it off! For now I will just summarise the derivation of a VAE as follows:
- As per maximum likelihood estimation, we wish to find parameters $\theta$ such that we maximise the log likelihood $\pt(\xx)$ over the data. To learn useful features about the data we can express it was a marginalisation over an additional latent variable $\zz$.
- The marginalisation is intractable, because $\pt(\xx) = \int_{\zz} \pt(\xx,\zz) d\zz$.
- If one factorises $\pt(\xx,\zz)$ into $\pt(\zz|\xx)p(\xx)$ then the above marginalisation could be approximated with Monte Carlo. However, we don't know that $\pt(\zz|\xx)$ should be since we don't know the true latent variables $\zz$. If we try derive it via Bayes' rule, i.e. $\pt(\zz|\xx) = \pt(\xx|\zz)p(\zz) / Z$, then we just have the same problem of intractability since $Z$ is the normalising constant. Furthermore, it still isn't clear how $\zz$ to be obtained, since one conditional depends on the other in a circular fashion.
- This issue can be resolved by substituting $\pt(\zz|\xx)$ with a separate network $\qp(\zz|\xx)$ and we derive a tractable objective for $\pt(\xx)$, which is called the evidence lower bound.

# In the next section I'll give a different perspective on how the ELBO can be derived. This perspective will help us reason about some of the difficulties inherent in training conditional VAEs. 

** Contributions

The contributions of this blog post are as follows:

- In Section [[#sec_perspective]] we present cVAEs through an unconventional but rather enlightening perspective, thanks to the work done by Esmaeili et al. in 2018. This involves thinking about the VAE as parameterising two separate pathways (the /generative/ and /inference/ process) and minimising the KL divergence between the two. While their work only looked at unconditional VAEs, we modify their derivations to also account for an extra conditioning variables $\yy$.
- We discuss two parameterisations of a cVAE: one where the conditioning variable $\yy$ and latent variable $\zz$ are assumed to be independent (Section [[#sec_zy_independent]]), and one where they are not (Section [[#sec_zy_dependent]]).
- Through the lens of mutual information terms, in Section [[#sec_mi]] we explain the difficult in training CVAEs, namely that the learning a generative model that is able to be conditioned on more than one latent variable inevitably degrades the sample quality of the entire model because of two competing terms in the model's loss function (the evidence lower bound, or 'ELBO'). For the remainder of this work we will call this the /ELBO dilemma/.
- In Section [[#sec_experiments]] we present experiments corroborating this behaviour on a toy 2D dataset consisting of two Gaussian clusters, where a conditional VAE must be trained to sample from either of the two clusters correctly. 
- We present a potential fix to the /ELBO dilemma/, as well as discuss why this issue does not exist for usual implementations of diffusion models, which can be seen as multi-latent generalisations of the VAE.

* The generative and inference process
:PROPERTIES:
:CUSTOM_ID: sec_perspective
:END:

Inspired by =esmaeili2018structured=, we can actually derive the ELBO for a VAE by framing the training objective as a minimisation over the KL divergence between two pathways, each encoded by their own joint distribution. Since we're also talking about conditional VAEs, we will be dealing with an additional latent variable $\yy$, but unlike $\zz$ we have labels for this. We assume that $\yy$ encodes some semantic meaningful label of interest, for instance the class of a digit, or the identity of an object. 

Before we derive the KL, let us explain the two pathways encoded by a VAE. The first pathway is the /inference process/, denoted $\qp(\xx,\zz,\yy)$. It factorises into $\qp(\zz|\xx,\yy)q(\xx,\yy)$, and to obtain a sample $(\zz,\xx,\yy)$ from this joint we simply perform the following:

\begin{align} \label{eq:inference}
\xx, \yy & \sim q(\xx, \yy) \ \ \text{(ground truth)} \tag{2a} \\
\zz & \sim  \qp(\zz|\xx, \yy) \tag{2b}
\end{align}

where $q(\xx,\yy)$ is the ground truth data distribution, and $\qzgivenx$ is our learnable variational posterior, subscripted with $\phi$. The inference process is concerned with extracting latent representations from actual samples from the data distribution. This is to be contrasted with the /generative/ process, in which samples are generated as the following:

\begin{align} \label{eq:generative}
\zz, \yy & \sim p(\zz,\yy) \tag{3a} \ \ \text{(prior)} \\
\xx &\sim \pt(\xx|\zz,\yy) \tag{3b},
\end{align}

where $p(\zz,\yy)$ is prescribed beforehand. (We will talk a little more about this shortly.) 

Since joint distribution for both processes are $\ptgreen(\xx,\zz,\yy)$ and $\qp(\xx,\zz,\yy)$ and we can derive their KL distribution as follows (note that it is framed as the argmax over the /negative KL/ to be consistent with =esmaeili2018structured=):

\begin{align} \label{eq:case1}
\argmax_{\color{green}{\theta}, \color{purple}{\phi}} & -\kldiv \Big[ \qp(\XX,\ZZ,\YY) \ \| \ \ptgreen(\XX,\ZZ,\YY) \Big] \\ 
& = \mathbb{E}_{\qp(\xx,\zz,\yy)}\big[ \log \frac{\pt(\xx,\zz,\yy)}{\qp(\xx,\zz,\yy)} \big] \tag{4a} \\
& = \mathbb{E}_{\qp(\zz|\xx,\yy)}\big[ \log \frac{\pt(\xx | \yy, \zz)p(\yy,\zz)}{\qp(\zz|\xx,\yy)} \big] - \mathbb{E}_{q(\xx,\yy)} \log q(\xx, \yy) \tag{4b} \\
& = \mathbb{E}_{\qp(\xx,\zz,\yy)}\big[ \log \frac{\pt(\xx | \yy, \zz)p(\yy, \zz)}{\qp(\zz|\xx,\yy)} \big] - \text{const.} \tag{4c} \\
& = \mathbb{E}_{\qp(\xx,\zz,\yy)} \big[ \log \pt(\xx|\yy,\zz) \big] + \mathbb{E}_{\qp(\zz|\xx,\yy)} \big[ \log \frac{p(\yy, \zz)}{\qp(\zz|\xx,\yy)} \big] - \text{const.} \tag{4d} \\
& = \mathbb{E}_{\qp(\zz,\xx,\yy)}\big[ \log \pt(\xx|\yy,\zz) \big] - \kldiv\Big[ \qp(\ZZ|\XX, \YY) \| p(\ZZ,\YY)\Big] - \text{const.} \tag{4e}
\end{align}
# qp(zz,xx,yy) on second to last line, second term (KL) is making this whole thing not render... why??

** When z and y are independent
:PROPERTIES:
:CUSTOM_ID: sec_zy_independent
:END:


At this point, we have to specify what $p(\zz,\yy)$ is, and we have two options. The first is to assume that $p(\zz,\yy) = p(\zz)p(\yy)$, i.e. they are independent. This means that the joint distribution of the generative process factorises into:

\begin{align}
\pt(\xx,\zz,\yy) = \pt(\xx|\zz,\yy)p(\zz)p(\yy) \tag{5}
\end{align}

which leads us to the following ELBO:

\begin{align}
& -\kldiv \Big[ \qp(\XX,\ZZ,\YY) \ \| \ \ptgreen(\XX,\ZZ,\YY) \Big] \tag{6a} \\ 
& \myeq{ind.} \mathbb{E}_{\qp(\zz,\xx,\yy)}\big[ \log \pt(\xx|\yy,\zz) \big] + \mathbb{E}_{\qp(\zz,\xx,\yy)}\big[ \log \frac{\pgreen(\zz)}{\qp(\zz|\xx,\yy)} \big] + \log \pgreen(\yy) \tag{6b} \\
& = \text{likelihood} - \kldiv\Big[ \qp(\ZZ|\XX,\YY) \| p(\ZZ) \Big] + \text{constants}. \tag{6c}
\end{align}

Here, $p(\yy)$ is some prior for $\yy$ but it falls out of the KL term since it is a constant, so we need not worry about it. All that is left is to define a prior for $p(\zz)$, and in practice this is most often an isotropic Gaussian distribution. The graphical model for the $\color{green}{\text{generative process}}$ is also shown in Figure 1.

#+BEGIN_EXPORT html
<div id="images">
<br />
<figure>
<img class="figg" src="/assets/cvae/cvae-independent.png" width="400" alt="" /> 
</figure>
<figcaption><b>Figure 1: Graphical model of the generative process, corresponding to the case where p(z,y) = p(z)p(y).</b></figcaption>
<br />
</div>
#+END_EXPORT

Such an assumption may be useful to encode if we are seeking to learn /disentangled/ representations. For instance, if we were learning a conditional VAE over SVHN digits (where $y$ encodes the identity of the digit), perhaps we would like for our VAE to learn a $\zz$ that encodes /everything else/ in the image apart from the digit itself, for instance details in the background and the font, colour, etc. of the digit. This would make for a very controllable generative process where we are able to apply different styles $\zz$ to the same digit $\yy$, and vice versa.

** When z and y are dependent
:PROPERTIES:
:CUSTOM_ID: sec_zy_dependent
:END:

 Otherwise, $\pgreen(\zz,\yy) = \pgreen(\zz|\yy)\pgreen(\yy)$ and $\pgreen(\zz|\yy)$ is the /conditional prior/. This means that the joint distribution factorises into:

\begin{align}
\pt(\xx,\zz,\yy) = \pt(\xx|\zz,\yy)p(\zz|\yy)p(\yy) \tag{7}
\end{align}

 The conditional prior can either be fixed (i.e. each possible value of $\yy$ gets mapped to a Gaussian), or it can be learned, in which case we denote it as $\pt(\zz|\yy)$. In this case the ELBO in Eqn. (4d) can be simplified down to:

\begin{align}
& -\kldiv \Big[ \qp(\XX,\ZZ,\YY) \ \| \ \ptgreen(\XX,\ZZ,\YY) \Big] \tag{8a} \\ 
& \myeq{dep.} \mathbb{E}_{\qp(\zz,\xx,\yy)}\big[ \log \pt(\xx|\yy,\zz) \big] + \mathbb{E}_{\qp(\zz,\xx,\yy)}\big[ \log \frac{p(\zz|\yy)}{\qp(\zz|\xx,\yy)} \big] + \log p(\yy) \tag{8b} \\
& = \text{likelihood} - \kldiv\Big[ \qp(\ZZ|\XX,\YY) \ \| \ p(\ZZ|\YY) \Big] + \text{constants}. \tag{8c}
\end{align}

Consequently, the graphical model for the $\color{green}{\text{generative process}}$ is shown in Figure 2.

#+BEGIN_EXPORT html
<div id="images">
<br />
<figure>
<img class="figg" src="/assets/cvae/cvae-dependent.png" width="400" alt="" /> 
</figure>
<figcaption>Figure 2: Graphical model of the generative process, corresponding to the case where p(z,y) = p(z|y)p(y).</figcaption>
<br />
</div>
#+END_EXPORT

** A mutual information interpretation
:PROPERTIES:
:CUSTOM_ID: sec_mi
:END:

This new ELBO can be written as a sum of four terms (for the full derivation, see Sec [[#sec_derivation]]):

\begin{align} \label{eq:elbo4}
\mathcal{L}_{ \theta, \psi}(\xx) = \mathbb{E}_{\qp(\xx,\yy,\zz)} \Big[ \underbrace{\log \frac{\pt(\xx|\zz,\yy)}{\pt(\xx)}}_{\circleone} - \underbrace{\log \frac{\qp(\zz|\xx, \yy)}{\qp(\zz)}}_{\circletwo} \Big] - \nonumber \\
\underbrace{\kldiv( \qpink(X) \| \pt(X) )}_{\circlethree} - \underbrace{\kldiv( \qp(Z) \| p(Z, Y))}_{\circlefour} \tag{9}
\end{align}

where $p(Z,Y) = p(Z)p(Y)$ when those variables are independent, otherwise $p(Z,Y) = p(Z|Y)p(Y)$. As stated in =esmaeili2018structured=, the $\beta$ weighted ELBO term which is commonly seen in VAE papers can be written as:

\begin{align}
\max_{\phip, \theta} \text{ELBO} & = \max_{\phip,\theta} \circleone+\circlethree + \beta \Big(\circletwo+\circlefour\Big) \tag{9b}  \\
& = \max_{\phip, \theta} \mathbb{E}_{\qp(\xx,\yy,\zz)} \underbrace{\log \frac{\pt(\xx|\zz,\yy)}{\pt(\xx)}}_{\circleone} + \underbrace{\kldiv( \qpink(X) \| \pt(X) )}_{\circlethree} + \nonumber \\
& \ \ \ \ \ \beta \Big( \underbrace{-I_{\phip}(\ZZ; \XX, \YY)}_{\circletwo} - \underbrace{\kldiv( \qp(Z) \| \pgreen(Z, Y))}_{  \circlefour} \Big) \tag{9c}
\end{align}




#+BEGIN_COMMENT
where $\beta$ is typically used to weight the KL term as in =burgess2018understanding=). =esmaeili2018structured= provides a very detailed explanation of all four of these terms and how they affect both the generative and inference model, as well as diagrams which illustrate what happens when each of the four terms are ommitted.
#+END_COMMENT

Since we typically /minimise/ loss functions, let us re-state Eqn. (9c) but as a minimisation, which means taking the negative of all terms:

\begin{align}
-\mathbb{E}_{\qp} \underbrace{-\log \frac{\pt(\xx|\zz,\yy)}{\pt(\xx)}}_{\circleone} - \underbrace{\kldiv( \qpink(X) \| \pt(X) )}_{\circlethree} + \beta \Big( \underbrace{I_{\phip}(\ZZ; \XX, \YY)}_{\circletwo} + \underbrace{\kldiv( \qp(Z) \| \pgreen(Z, Y))}_{  \circlefour} \Big) \tag{9d}
\end{align}

Let us focus on the two terms $\circleone$ and $\circletwo$. We'll start with $\circletwo$.



#+BEGIN_COMMENT
$\circleone$ encourages a bijective mapping between $\mathcal{X}$ and $\mathcal{Z}$, i.e. each $\xx$ should map to a unique $\zz$. This, combined with $\circlethree$ make up the likelihood term you commonly see in the two-term variant of the ELBO. $\circletwo$ 
#+END_COMMENT

** The "ELBO dilemma"
:PROPERTIES:
:CUSTOM_ID: elbo_dilemma
:END:

$\circletwo$ is minimising the /mutual information/ between $X$ and the pair $(Z,Y)$, which we denote $I_{\phip}(Z; X, Y)$ (note the use of the semicolon here to separate out the two sets of variables). We can show this via the following derivations:

\begin{align} \label{eq:expand2}
\max_{\phip} \beta \cdot \circletwo & = \min_{\phip} \beta \cdot I_{\phip}(\ZZ; \XX, \YY) \tag{10c} \\
 & = \min_{\phip} \beta \cdot \mathbb{E}_{\qp(\xx,\zz,\yy)} \Big[ \log \frac{\qp(\zz|\xx,\yy)}{\qp(\zz)} \Big] \tag{10a}  \\
& = \min_{\phip} \beta \cdot \Big( I_{\phip}(\ZZ; \XX) + I_{\phip}(\ZZ; \YY) + I_{\phip}(\XX; \YY; \ZZ) \Big), \tag{10d}
\end{align}


The two main terms of interest to us are $I_{\phi}(Z; X)$ and $I_{\phi}(Z; Y)$ (the third term is called interaction information and has a bit of a trickier interpretation, and I will leave that to this [[https://en.wikipedia.org/wiki/Interaction_information][wiki page]]). For a $Z,Y$ disentangled VAE (Sec. [[#sec_zy_dependent]]), $I_{\phip}(\ZZ; \YY)$ is a property we would like to /minimise/ because we want them to encode completely independent concepts. That is, we would like $\YY$ to encode some kind of semantic content about the image (e.g. the label), and $\ZZ$ to capture every other source of variation. In practice, if $\zz$ contains too much information about $\yy$ then it may make little to no difference in the generative path, resulting in the following degenerate scenario:

\begin{align}
\pt(\xx|\yy,\zz) = \pt(\xx | \yy', \zz) \ \ \forall \yy' \neq \yy. \tag{11}
\end{align}

and this can make it difficult for us to perform different kinds of 'controllable generation'. For instance, suppose for a given pair $(\xx,\yy)$ we want to find its latent code -- that is, isolate just the 'non-semantic' latent factors -- but decode with a different label, we can perform the following:

\begin{align}
(\xx, \yy) & \sim \mathcal{D} \tag{12a} \\
\yy' & \sim p(\yy) \tag{12b} \\
\zz & \sim \qp(\zz|\xx,\yy) \tag{12c} \\
\xx' & \sim \pt(\xx|\zz,\yy') \tag{12d}
\end{align}

But due to Eqn. (11) the new $\yy'$ won't make any difference whatsoever. We could measure the extent to how bad this is by simply computing (for a fixed $\xx$ and $\zz$):

\begin{align}
\mathbb{E}_{\yy, \yy' \sim p(\yy)} (\pt(\xx|\yy,\zz) - \pt(\xx|\yy',\zz))^{2},
\end{align}

that is, an average over the difference in the decoder for different pairs of $\yy$. If on average the pairs are have very small L2 distance then this indicates that $\yy$ has very little influence on the generative model $\pt$.

The reason for this is due to another term in the ELBO working against us. Note the term $\circleone$ in Eqn. (9): maximising this means maximising the conditional likelihood of $\xx|\zz$ with respect to samples $\zz$ from the inference process, and this requires that there be high mutual information between $\xx$ and $\zz$. This is because $\circleone$ is actually an approximation to the mutual information between $\XX$ and $\ZZ$:

#+BEGIN_COMMENT
At the same time, $I_{\phip}(\ZZ; \XX)$ is also in Eqn. (5d) and that is being minimised. This term is /constraining/ how much information about $\XX$ is encoded in $\ZZ$, and this in turn will negatively affect our ability to reconstruct the data well. 
#+END_COMMENT


\begin{align}
\max_{\thetagr} I_{\thetagr}(\XX; \ZZ, \YY) & = \max_{\thetagr} I_{\thetagr}(\XX; \ZZ) + I_{\thetagr}(\XX; \YY) + I_{\thetagr}(\XX; \ZZ; \YY) \tag{12a} \\ 
& = \mathbb{E}_{\pt(\xx,\zz,\yy)} \log \frac{\pt(\xx|\zz,\yy)}{\pt(\xx)} \tag{12b} \\
& \approx \underbrace{\mathbb{E}_{\qp(\xx,\zz,\yy)} \log \frac{\pt(\xx|\zz,\yy)}{\pt(\xx)}}_{\circleone} \tag{12c}
\end{align}

I am calling Eqn. (12c) "approximate mutual information", and it would only be equivalent to the real mutual information if the generative process $\ptgreen(\xx,\yy,\zz)$ was equivalent to $\qp(\xx,\yy,\zz)$, as shown in (12b). As per Eqn. (9d), smaller values of $\beta$ put more relative weight on term $\circleone$, which means $\ZZ$ gets to encode more information about $\XX$. When the $\ZZ$ variable isn't constrained with what information it can encode about $\XX$, it can also encode the sorts of things that should really be encoded by the variable $\YY$, making $\YY$ redundant in the process. This puts us in a dilemma however: we want $\YY$ to encode semantic information and $\ZZ$ to encode all other factors of variation, but the more we want this to be likely the more we need to increase $\beta$, which subsequently cripples the ability for $\ZZ$ to encode the 'all other factors of variation'. In practice, we may be able to train a cVAE with $\ZZ$ and $\YY$ being disentangled but at the expense of blurry samples. (Note that this issue is also relevant even when [[#sec_zy_dependent][z and y are dependent]]. In that particular case, $\ZZ$ and $\YY$ are not independent but you still don't the latter to be ignored by $\pt$.)

Furthermore, when we train cVAEs, it is common to monitor the ELBO on a held-out validation set and use it to perform hyperparameter tuning and early stopping. However, the ELBO does /not/ make it clear if the $\YY$ variable is being ignored or not. Either one has to eye-ball the generated samples or implement and monitor Eqn. (11) over the course of training. To the best of my knowledge I have not seen something like Eqn. (11) proposed in the literature, but I personally found it useful in my own experiments.

#+BEGIN_COMMENT
I argue that even when [[#sec_zy_dependent][z and y are dependent]], $\beta$ should be large enough such that we reduce the mutual information between $\XX$ and $\ZZ$, so that we do not risk the inference network ignoring the $\YY$ variable. While Eqn. (5d) does also contain $I_{\phip}(\ZZ; \YY)$ this is with respect to the /inference process/ (hence the subscripted $\phip$), not with respect to the generative process $I_{\thetagr}(\ZZ; \YY)$. If we were to minimise $I_{\thetagr}(\ZZ; \YY)$ we would indeed be forcing $\ptgreen(\zz|\yy)$ to collapse to $p(\zz)$ for all $\yy$, and that is certainly not what we want.
#+END_COMMENT



* Experiments
:PROPERTIES:
:CUSTOM_ID: sec_experiments
:END:

Here we present some experiments on a toy 2D dataset, which suffice to illustrate the issues in training cVAEs. The dataset consists of two Gaussians, and the ground truth can be seen as $p(\xx) = \sum_{i=0,1}p(\xx,\yy) = \sum_{i=0,1} p(\xx|\yy)p(\yy=i)$, where:

- $p(\xx|\yy=0) = \mathcal{N}(\xx; [-2.5, 1]^{T}, 2\mathbf{I})$, 
- $p(\xx|\yy=1) = \mathcal{N}(\xx; [6,-2]^{T}, 2 + \mathbf{I})$, and 
- $p(\yy=0) = p(\yy=1) = \frac{1}{2}$.

Samples from this distribution are visualised below in Figure 3.

#+BEGIN_EXPORT html
<div id="images">
<br />
<figure>
<img class="figg" src="/assets/cvae/toy_dataset.png" width="500" alt="" /> 
</figure>
<figcaption><i>Figure 3: Illustration of the toy 2D dataset used. The dataset comprises of two Gaussians, each corresponding to one of two binary labels (y=0 or y=1).</i></figcaption>
<br />
</div>
#+END_EXPORT

** When z and y are independent
:PROPERTIES:
:CUSTOM_ID: sec_exps_zy_independent
:END:

First we show $\beta = 0$, illustrated in Figure 3. Samples from the inference process are shown in $\color{purple}{\text{purple}}$ and those from the generation process in $\color{green}{\text{green}}$, similar to the notation that we have been using so far in the equations. For instance if we consider the inference process: for a given $(\xx, \yy)$ from the data distribution, we sample $\zz \sim \qp(\zz|\xx,\yy)$ and then we reconstruct by sampling $\tilde{\xx} \sim \pt(\xx|\zz,\yy)$. The corresponding reconstruction error is shown in the title (the squared L2 norm between the original points and their reconstructions), and we can see that the error is small enough we can essentially consider it to be zero. However, things don't look so good for the generative process: for a given $\zz \sim p(\zz)$, we can either choose to decode with $\pt(\xx|\zz,\yy=0)$ or $\pt(\xx|\zz,\yy=1)$, and these more or less fall in the same region. This indicates that choosing $\yy$ does not make a difference to the generated samples. What we would like to see is each group of green points (the green circles and crosses) being assigned to their respective clusters. 

#+BEGIN_EXPORT html
<div id="images">
<br />
<figure>
<img class="figg" src="/assets/cvae/vae_2d_beta0.png" width="700" alt="" /> 
</figure>
<figcaption><b><i>Figure 3: The case where p(z,y) = p(z)p(y). Here, there is no weight on the KL term, so reconstructions are good and there so is the inference process. However, samples from p(z,y=0) or p(z,y=1) (when decoded) fall in the same region (the green circles/crosses). Overall, with respect to the generative process, sample quality and sample diversity are bad.</i></b></figcaption>
<br />
</div>
#+END_EXPORT

We can relate this back to what we discussed in Sec. [[#elbo_dilemma]]. Since there is no weighting on term $\circletwo$, there is nothing constraining the amount of information about $\yy$ to be encoded in $\zz$, and therefore $\yy$ gets ignored by the model. However, we haven't quite discussed term $\circlefour$ which also being multiplied by the $\beta$ term. Essentially, $\circlefour$ is measuring the distance (via the KL divergence) between the /marginal inference posterior/:

\begin{align}
\qp(\zz) & = \int_{\xx,\yy} \qp(\zz|\xx,\yy)q(\xx,\yy) \ d\xx d\yy = \mathbb{E}_{q(\xx,\yy)} \ \qp(\zz|\xx,\yy),
\end{align}

i.e. the average of $\qp(\zz|\xx,\yy)$ with respect to $(\xx,\yy)$'s sampled from the data distribution, and the prior distribution $p(\zz, \yy)$. Since $p(\zz,\yy) = p(\zz)p(\yy)$ here, Eqn. (9d) can be simpified to the KL between $\qp(\zz)$ and $p(\zz)$ and $p(\yy)$ becomes a constant. Since $\beta = 0$ here, there is no incentive for $\qp$ to match the prior, and so it shouldn't be surprising that the purple and green points don't match each other in shape. Since the $\zz$ space is also two-dimensional, we can also visualise points from both clusters in that space, and this is shown in Figure 3b. If we were to draw a contour which encompassed all of the purple points, we could view it as $\qp(\zz)$ our marginal posterior.

#+BEGIN_EXPORT html
<div id="images">
<br />
<figure>
<img class="figg" src="/assets/cvae/vae_2d_beta0_zspace.png" width="700" alt="" /> 
</figure>
<figcaption><b><i>Figure 3b: beta = 0, showing samples in z space, which is also two-dimensional. The prior distribution p(z) is shown as the green sphere. The radius of the sphere corresponds to one standard deviation.</i></b></figcaption>
<br />
</div>
#+END_EXPORT


In Figure 4, if we choose $\beta = 0.01$, it looks as though some of the green points have been pulled to their respective cluster but there is still some overlap between the two categories and we don't see any clear pattern of separation. At the very least, sample diversity is superior to that in Figure 1 because at least the green points are sufficiently spread out to cover the two clusters of the data. The reconstruction error for the inference process has only taken a minor hit, increasing from roughly zero to $\approx 0.02$.

#+BEGIN_EXPORT html
<div id="images">
<br />
<figure>
<img class="figg" src="/assets/cvae/vae_2d_beta-large.png" width="700" alt="" /> 
</figure>
<figcaption><b><i>Figure 4: The case where p(z,y) = p(z)p(y). Î² = 0.01, so reconstructions are decent and there so is the inference process. Samples from the generative process still do not appear to respect their clusters but unlike Figure 1 we see an acceptable level of sample diversity here, since those samples are covering more regions of the data distribution. Overall, with respect to the generative process, sample quality is bad but sample diversity is good.</i></b></figcaption>
<br />
</div>
#+END_EXPORT

Finally, in Figure 5 for $\beta = 1$  we finally see that the green points get matched to their respective clusters. Unfortunately, the inference process has degraded and reconstruction error has significantly increased as as result ($\approx 1.61$). We can also see this qualitatively for the orange cluster, where reconstructions lie on a very narrow subspace instead of being more evenly distributed across the cluster. Compared to the previous experiment, sample quality is /very good/ but sample diversity has degraded.

#+BEGIN_EXPORT html
<div id="images">
<br />
<figure>
<img class="figg" src="/assets/cvae/vae_2d_beta-large2.png" width="700" alt="" /> 
</figure>
<figcaption><b><i>Figure 6: Graphical model of the generative process, corresponding to the case where p(z,y) = p(z)p(y).</i></b></figcaption>
<br />
</div>
#+END_EXPORT

** Controllable generation
:PROPERTIES:
:CUSTOM_ID: sec_exps_controllable
:END:

One benefit of training a $\ZZ,\YY$ independent VAE is that we can perform /controllable/ generation more easily. For instance, we may wish to encode $\xx$ into two or more independent latent variables, substitute one of those variables' values, and then decode to produce a new example. For our 2D dataset we could for instance encode a pair $(\xx,\yy)$ into a $\zz$ but /decode/ with the opposite label when performing generation. That is:

\begin{align}
(\xx,\yy) & \sim \mathcal{D} \tag{13a} \\
\zz & \sim \qp(\zz|\xx,\yy) \tag{13b} \\
\xx' & \sim \pt(\xx|\zz,1-\yy) \tag{13c}
\end{align}

In Sec. [[#elbo_dilemma]] we discussed how the 'ELBO dilemma' may also make this task difficult. Therefore, similar to Sec. [[#sec_exps_zy_independent]] let us try this with increasing values of $\beta$ starting from zero. See Figures 7(a,b,c) and their associated captions.

#+BEGIN_EXPORT html
<div id="images">
<br />
<figure>
<img class="figg" src="/assets/cvae/vae_2d_beta0_swapped.png" width="700" alt="" />
</figure>
<figcaption><b><i>Figure 7a: beta = 0. Label swapping doesn't seem to do anything.</i></b></figcaption>
<br />
<figure>
<img class="figg" src="/assets/cvae/vae_2d_beta-large_swapped.png" width="700" alt="" />
</figure>
<figcaption><b><i>Figure 7b: Label swapping has an effect but sometimes samples lie in between the two clusters.</i></b></figcaption>
<br />
<figure>
<img class="figg" src="/assets/cvae/vae_2d_beta-large2_swapped.png" width="700" alt="" />
</figure>
<figcaption><b><i>Figure 7c: Label swapping looks like it works now, albeit at the cost of sample diversity for the right-most cluster.</i></b></figcaption>
<br />
</div>
#+END_EXPORT

#+BEGIN_COMMENT
As we mentioned in Section [[#sec_mi]], the reason for this is because smaller values of $\beta$ put too much relative weight on $\circleone$, which is (approximately) maximising the mutual information between $\zz$ and $\xx$. If $\zz$ contains enough information about $\yy$ (through inferring that information about $\xx$) then $\yy$ simply gets ignored during decoding because it isn't necessary to consider. In order to stop this from happening, $\zz$ needs to contain as little information about $\yy$ as possible, and this happens for large values of $\beta$ via $\circletwo$.
#+END_COMMENT

** When z and y are dependent
:PROPERTIES:
:CUSTOM_ID: sec_exps_zy_dependent
:END:

When $\zz$ and $\yy$ are dependent then $p(\zz,\yy) = p(\zz|\yy)p(\yy)$. Either we fix the conditional prior $p(\zz|\yy)$ a-priori and manually define both $p(\zz|\yy=0)$ and $p(\zz|\yy=1)$, or we learn the conditional prior instead, in which case we can substitute the term with $\pt(\zz|\yy)$ instead. Learning the conditional prior simply means including four extra parameters in $\theta$ that comprise the mean and variance of the Gaussians corresponding to $\yy=0$ and $\yy=1$.

In Figures 8(a,b,c) we produce similar plots to that of Sec. [[#sec_exps_zy_independent]].

#+BEGIN_EXPORT html
<div id="images">
<br />
<figure>
<img class="figg" src="/assets/cvae/cond_prior/vae_2d_beta0.png" width="700" alt="" />
</figure>
<figcaption><b><i>Figure 8a: beta = 0 with the learned conditional prior.</i></b></figcaption>
<br />
<figure>
<img class="figg" src="/assets/cvae/cond_prior/vae_2d_beta0.01.png" width="700" alt="" />
</figure>
<figcaption><b><i>Figure 8b: beta = 0.01 with the learned conditional prior.</i></b></figcaption>
<br />
<figure>
<img class="figg" src="/assets/cvae/cond_prior/vae_2d_beta1.png" width="700" alt="" />
</figure>
<figcaption><b><i>Figure 8c: beta = 1.0 with the learned conditional prior.</i></b></figcaption>
<br />
</div>
#+END_EXPORT

We also show an additional plot showing what the samples look like in /latent space/, as well as where the learned conditional priors $\pt(\zz|\yy=0)$ and $\pt(\zz|\yy=1)$ are located.

The key here is another term in the ELBO, which is term (d) in Eqn. (9). Re-writing it here:

\begin{align}
\max_{\phip} - \mathcal{D}_{\text{KL}}(\qp(\ZZ) \| p(\ZZ|\YY)) & = \min_{\phip} \mathcal{D}_{\text{KL}}(\qp(\ZZ) \| p(\ZZ|\YY)) \\
& = \min_{\phip} \mathbb{E}_{\qp(\zz,\yy)} \log \frac{\qp(\zz)}{p(\zz|\yy)}.
\end{align}

Note that since we learn the conditional prior here as part of the generative parameters $\theta$, let us substitute in $\pt(\zz|\yy)$ instead of $p(\zz|\yy)$:

\begin{align}
\min_{\phip,\theta} \mathcal{D}_{\text{KL}}(\qp(\ZZ) \| \pt(\ZZ|\YY)) = \mathbb{E}_{\qp(\zz,\yy)} \log \frac{\qp(\zz)}{\pt(\zz|\yy)}.
\end{align}


This is simply enforcing that the conditional prior -- averaged over the different realisations of $\yy$ -- is close to the /marginal/ inference posterior $\qp(\zz)$, which has been marginalised over all possible realisations of $\xx$ and $\yy$, which is simply $\int_{\xx,\yy} \qp(\zz|\xx,\yy) q(\xx,\yy) d\xx d\yy$. (Don't worry, we don't actually have to ever compute this, this is just a statistical definition of $\qp(\zz)$.)

If we assume that $\qp(\zz)$ covers a large space in $\mathcal{Z}$, it wouldn't make sense for the conditional prior to occupy small regions of that space -- they would have to be spread out to cover the variation exhibited in $\qp(\zz)$. Empirically, as we increase $\beta$ this is what we see.
























** Tuning beta
:PROPERTIES:
:CUSTOM_ID: sec_exps_tuning_beta
:END:

While it may look as though choosing a value of $\beta$ in between $(0.01, 1)$ may alleviate these concerns and give us the right balance between a good generative and inference process, it is still not easy to achieve in practice. In my own experience -- for image datasets as 'toy' as SVHN or EMNIST -- achieving this balance was almost impossible, even for what was a state-of-the-art VAE at the time (the hierarchical VAE proposed by =child2020very=). In fact, to get an even remotely acceptable balance between the two I had to significantly increase the capacity of the VAE, so much that I was training VAEs with at least a 100M parameters. Not only was this bizare for such a supposedly simple set of datasets, I simply couldn't justify using that much GPU compute on something so (supposedly) trivial.

Here is an artifact from that generative modelling project. We were trying to do style/content swaps for images from SVHN -- here, one can think of the content as being $\yy$, the identity of the SVHN digit. For each row:
- =x1= is $\xx_1$, =x2= is $\xx_2$. Their corresponding labels are the digits, e.g. $\yy_1$ will be 18. $\yy_2$ depends on what column we are looking at.
- =recon= is the reconstruction of $\xx_1$, as per the inference process.
- =x1_c, x2_s= says: take the content of $\xx_1$ and the style from $\xx_2$. This means, we sample $\xx \sim \ptgreen(\xx|\yy_1,\zz_2)$, where $\yy_1$ is the identity of $\xx_1$, and $\zz_2 \sim \qp(\zz|\yy_2,\xx_2)$.
- =x2_c, x1_s= says the opposite: take the /content/ of $\xx_2$ and the style from $\xx_1$. This means, we sample $\xx \sim \ptgreen(\xx|\yy_2,\zz_1)$, where $\yy_2$ is the identity of $\xx_2$, and $\zz_1 \sim \qp(\zz|\yy_1,\xx_1)$.


#+BEGIN_EXPORT html
<div id="images">
<br />
<figure>
<img class="figg" src="/assets/cvae/content-style-swap.png" width="700" alt="" /> 
</figure>
<figcaption><b>Figure 7: An example of a (z,y)-disentangled conditional VAE trained on a modified version of SVHN.</b></figcaption>
<br />
</div>
#+END_EXPORT


* Discussion

How do we resolve this? I think one way we can do so...

Consider the case: $p(\yy,\zz) = p(\zz|\yy)p(\yy)$. To re-iterate the ELBO from Eqn. (8b): 

\begin{align}
\mathbb{E}_{\qp(\zz,\xx,\yy)}\big[ \log \pt(\xx|\yy,\zz) \big] + \mathbb{E}_{\qp(\zz,\xx,\yy)}\big[ \log \frac{p(\zz|\yy)}{\qp(\zz|\xx,\yy)} \big] + \log p(\yy)
\end{align}

where we have a /conditional prior/ $p(\zz|\yy)$, and the same constant $\log p(\yy)$ term as we did earlier. As mentioned earlier in this post, I claimed that the issue of $\yy$ being ignored by the network inevitably happens whether you use the independent or dependent formulation, even though I only demonstrated experiments for the independent case. In the case of dependence, the degenerate case amounts to $p(\zz|\yy) \approx p(\zz)$, which means that $\yy$ has no effect on the conditional prior.

What I propose is, instead of explicitly defining the conditional prior $\pt(\zz|\yy)$, derive it with the combination of Bayes' rule:

\begin{align}
\pt(\zz|\yy) = \frac{\pt(\yy|\zz)p(\zz)}{p(\yy)}
\end{align}

What is interesting is that this now becomes a function of $\ptgreen(\yy|\zz)$, which looks a lot like a classifier, i.e., it is tasked with predicting the distribution over $\yy$ given the latent code $\zz$. To see why this can potentially be useful, let us plug it into the numerator of the ELBO:

\begin{align}
& \mathbb{E}_{\qp(\zz,\xx,\yy)} \big[ \log \ptgreen(\xx|\yy,\zz) \big] + \mathbb{E}_{\qp(\zz,\xx,\yy)} \big[ \log \frac{p(\zz|\yy)}{\qp(\zz|\xx,\yy)} \big] \nonumber \\
& = \text{likelihood} + \mathbb{E}_{\qp}\big[ \log \frac{p(\zz|\yy)}{q(\zz|\xx,\yy)} \big] \nonumber \\
    & = \text{likelihood} + \mathbb{E}_{\qp}\big[ \log \frac{ \frac{\pt(\yy|\zz)p(\zz)}{p(\yy)} }{\qp(\zz|\xx,\yy)} \big] \ \ \text{(Bayes' rule)} \nonumber \\
    & = \text{likelihood} + \mathbb{E}_{\qp}\big[ \log \frac{ \ptgreen(\yy|\zz)p(\zz) }{q(\zz|\xx,\yy)p(\yy)} \big] \nonumber \\
    & = \text{likelihood} + \mathbb{E}_{\qp}\big[ \log \frac{ p(\zz) }{\qp(\zz|\xx,\yy)} \big] + \mathbb{E}_{\qp}\big[ \log \frac{ \pt(\yy|\zz) }{p(\yy)} \big] \nonumber \\
    & = \text{likelihood} + \mathbb{E}_{\qp}\big[ \log \frac{ p(\zz) }{\qp(\zz|\xx,\yy)} \big] + \mathbb{E}_{\qp}\big[ \log \pt(\yy|\zz) \big] - \log p(\yy) \nonumber \\
    & = \text{likelihood} - \underbrace{\text{KL}\big[ \qp(\ZZ|\XX,\YY) \| p(\ZZ) \big]}_{\text{uncond. prior}} + \underbrace{\mathbb{E}_{\qp}\big[ \log \pt(\yy|\zz) \big]}_{\text{classifier}} - \underbrace{\log p(\yy)}_{\text{constant}}.
\end{align}

Just like with the likelihood term, in practice we can just perform a one-sample approximation for the classifier term. How do we sample from the conditional prior $\pt(\zz|\yy)$ however? I propose the use of SGLD \cite{welling2011bayesian} for this. We would like to sample from $\pt(\zz|\yy) \propto \pt(\yy|\zz)p(\zz)$, and we can do this via an iterative process that computes gradients and injects noise:

\begin{align}
\zz_{0} & \sim p(\zz) = \mathcal{N}(0,1) \\
\yy & \sim p(\yy) \\
\zz_{t+1} & = \frac{\epsilon_t}{2} \Big( \nabla_{\zz} \log p(\zz) + \nabla_{\zz} \log \pt(\yy|\zz) \Big) + \eta_{t}, \ \ t \in \{1, \dots, T\}
\end{align}

where $\eta_{t} \sim \mathcal{N}(0, \epsilon_t)$, and $\zz_{T}$ should more or less be a draw from $\pt(\zz|\yy)$ for a reasonably large $T$.

** Does this happen with diffusion models?

Diffusion models can be seen as multi-latent generalisations of VAEs =ho2020diffusion=. Instead of just a single latent variable $\zz$, we have many noisy versions of $\xx$ which we denote $\xx_1, \dots, \xx_T$ for $T$ denoising diffusion timesteps. The only caveat is that these variables all have the same dimension as $\xx_0$.

The first thing that is worth mentioning is that typical implementations of conditional diffusion models are /not/ derived from the conditional ELBO like it is typically done with VAEs. Since the unconditional ELBO is used, there is no dependence on $\yy$ and therefore the ELBO dilemma is avoided completely: TODO.

Usually, an unconditional diffusion model $\pt(\xx)$ is trained and then it is guided with an external classifier. This is possible because diffusion models learn the score $\nabla_{\xx} \log \pt(\xx)$, and this doesn't require estimating a normalising constant. Therefore, the score can be easilybe  transformed into a joint density $\pt(\xx,\yy)$. For instance, given some external classifier $p_{\psi}(\yy|\xx)$ with parameters $\psi$ we could define the joint density as follows:

\begin{align}
\pt(\xx,\yy) = \frac{1}{Z_{\theta,\phi}}\pt(\xx)p_{\psi}(\yy|\xx)^{w} \tag{18}
\end{align}

for some unknown normalising constant $Z_{\theta,\phi}$ and hyperparameter $w$. However, taking the derivative of the log of Eqn. X with respect to $\xx$ (the score) gives us:

\begin{align}
\nabla_{\xx} \log \ptpgreen(\xx,\yy) & = \nabla_{\xx}\big[ \log \pt(\xx) + w \cdot \log  p_{\phi}(\yy|\xx) \big] \tag{19a} \\
& = \nabla_{\xx} \log \pt(\xx) + w \cdot \nabla_{\xx} \log  p_{\phi}(\yy|\xx). \tag{19b}
\end{align}






# https://writequit.org/articles/emacs-org-mode-generate-ids.html

* Appendix
:PROPERTIES:
:CUSTOM_ID: sec_appendix
:END:

** Derivation of Esmaeli's joint KL
:PROPERTIES:
:CUSTOM_ID: sec_derivation
:END:

Here we derive the main equation presented in =esmaeili2018structured=. This corresponds to the unconditional VAE, without $\yy$ conditioning.

\begin{align}
\color{green}{\theta}, \color{purple}{\phi} & = \argmax_{\color{green}{\theta}, \color{purple}{\phi}} -\mathcal{D}_{\text{KL}}\Big[ \qp(\ZZ,\XX) || \pgreen(\ZZ, \XX) \Big] \tag{10a} \\
& = \mathbb{E}_{\qzx} \Big[ \log \frac{\pzx}{\qzgivenx q(\xx)} \Big] \tag{10b} \\
& = \mathbb{E}_{\qzx} \Big[ \log \frac{\pxgivenz p(\zz)}{\qzgivenx q(\xx)} \Big] \tag{10c} \\
&  = \mathbb{E}_{\qzx} \Big[ \log \frac{\pxgivenz p(\zz)}{\qzgivenx q(\xx)} \cdot \frac{\ptgreen(\xx)}{\ptgreen(\xx)} \cdot \frac{\qp(\zz)}{\qp(\zz)} \Big] \tag{10d} \\
& = \mathbb{E}_{\qzx} \Big[ \log \frac{\pxgivenz}{\ptgreen(\xx)} + \log \frac{\qp(\zz)}{\qzgivenx} + \log \frac{\ptgreen(\xx)}{q(\xx)} + \log \frac{p(\zz)}{\qp(\zz)} \Big] \tag{10e} \\
& = \mathbb{E}_{\qzx} \Big[ \log \frac{\pxgivenz}{\ptgreen(\xx)} + \log \frac{\qp(\zz)}{\qzgivenx} \Big] + \mathbb{E}_{q(\xx)} \Big[ \log \frac{\ptgreen(\xx)}{q(\xx)} \Big] + \\
& \ \ \ \ \mathbb{E}_{\qp(\zz)} \Big[ \log \frac{p(\zz)}{\qp(\zz)} \Big]  \tag{10f} \\
& = \mathbb{E}_{\qzx} \Big[ \underbrace{\log \frac{\pxgivenz}{\ptgreen(\xx)}}_{\circleone} - \underbrace{\log \frac{\qzgivenx}{\qp(\zz)}}_{\circletwo} \Big] - \underbrace{\kldiv\Big[ q(\XX) \| \ptgreen(\XX) \Big]}_{\circlethree} - \\
& \ \ \ \ \ \underbrace{\kldiv\Big[ \qp(\ZZ) \| p(\ZZ)}_{\circlefour} \Big], \tag{10g}
\end{align}

where:
- $\ptgreen(\xx) = \int_{\zz} \ptgreen(\xx|\zz)p(\zz) d \zz$, the marginal distribution of the data /with respect/ to the /generative process/. This is also called the /marginal likelihood/.
- $\qp(\zz) = \int_{\xx} \qp(\zz|\xx)q(\xx) d\xx$ , the marginal distribution over the latent code /with respect to the inference process/. This is also called the /inference marginal/.

** Conditional case

We can derive the conditional case by adding $\yy$ wherever it is necessary. Starting from Eqn. (10f), we derive the following:

\begin{align}
& \mathbb{E}_{\qp(\zz,\xx,\yy)} \Big[ \log \frac{\ptgreen(\xx|\zz,\yy)}{\ptgreen(\xx)} - \log \frac{\qp(\zz|\xx,\yy)}{\qp(\zz)} \Big] + \mathbb{E}_{q(\xx)} \Big[ \log \frac{\ptgreen(\xx)}{q(\xx)} \Big] + \mathbb{E}_{\qp(\zz,\yy)} \Big[ \log \frac{p(\zz,\yy)}{\qp(\zz)} \Big]. \tag{11a}
\end{align}

We can subsequently refine this equation depending on the factorisation of $p(\zz,\yy)$, which we do below.

*** *z and y are independent*

For the sake of space, I will simply use $\qp$ to refer to the full joint distribution $\qp(\zz,\xx, \yy)$. For $p(\zz,\yy) = p(\zz)p(\yy)$, we get:

\begin{align}
& \mathbb{E}_{\qp} \Big[ \log \frac{\ptgreen(\xx|\zz,\yy)}{\ptgreen(\xx)} - \log \frac{\qp(\zz|\xx,\yy)}{\qp(\zz)} \Big] + \mathbb{E}_{q(\xx)} \Big[ \log \frac{\ptgreen(\xx)}{q(\xx)} \Big] + \\
& \ \ \ \ \ \mathbb{E}_{\qp(\zz,\yy)} \Big[ \log \frac{p(\zz)}{\qp(\zz)} + \log p(\yy) \Big] \tag{12a} \\
& = \mathbb{E}_{\qp} \Big[ \log \frac{\ptgreen(\xx|\zz,\yy)}{\ptgreen(\xx)} - \log \frac{\qp(\zz|\xx,\yy)}{\qp(\zz)} \Big] + \mathbb{E}_{q(\xx)} \Big[ \log \frac{\ptgreen(\xx)}{q(\xx)} \Big] + \\
& \ \ \ \ \ \mathbb{E}_{\qp(\zz)} \Big[ \log \frac{p(\zz)}{\qp(\zz)} \Big] + \mathbb{E}_{q(\yy)} \log p(\yy) \tag{12b} \\
& = \mathbb{E}_{\qp} \Big[ \underbrace{\log \frac{\ptgreen(\xx|\zz,\yy)}{\ptgreen(\xx)}}_{\circleone} - \underbrace{\log \frac{\qp(\zz|\xx,\yy)}{\qp(\zz)}}_{\circletwo} \Big] - \underbrace{\kldiv\Big[ q(\XX) \| \ptgreen(\XX) \Big]}_{\circlethree} \\
& \ \ \ \ \ - \underbrace{\kldiv\Big[ \qp(\ZZ) \| p(\ZZ)}_{\circlefour} \Big] + \text{const.} \tag{12c}
\end{align}

Here, $p(\yy)$ can fall out of the optimisation since it's just a constant. However, since it's a prior we can set it to whatever it is we want it to be, either the actual empirical distribution of $\yy$ for our dataset or another distribution.

*** *z and y are dependent*

Again, starting from Eqn. (10f), if we assume that $p(\zz,\yy) = p(\zz|\yy)p(\yy)$ then:

\begin{align}
& \mathbb{E}_{\qp(\zz,\xx,\yy)} \Big[ \log \frac{\ptgreen(\xx|\zz,\yy)}{\ptgreen(\xx)} - \log \frac{\qp(\zz|\xx,\yy)}{\qp(\zz)} \Big] + \mathbb{E}_{q(\xx)} \Big[ \log \frac{\ptgreen(\xx)}{q(\xx)} \Big] + \\
& \ \ \ \ \mathbb{E}_{\qp(\zz,\yy)} \Big[ \log \frac{p(\zz|\yy)p(\yy)}{\qp(\zz)} \Big] \tag{13a} \\
& \mathbb{E}_{\qp(\zz,\xx,\yy)} \Big[ \log \frac{\ptgreen(\xx|\zz,\yy)}{\ptgreen(\xx)} - \log \frac{\qp(\zz|\xx,\yy)}{\qp(\zz)} \Big] + \mathbb{E}_{q(\xx)} \Big[ \log \frac{\ptgreen(\xx)}{q(\xx)} \Big] + \\
& \ \ \ \ \mathbb{E}_{\qp(\zz,\yy)} \Big[ \log \frac{p(\zz|\yy)}{\qp(\zz)} + \log p(\yy) \Big]. \tag{13b} \\
& \mathbb{E}_{\qp(\zz,\xx,\yy)} \Big[ \log \frac{\ptgreen(\xx|\zz,\yy)}{\ptgreen(\xx)} - \log \frac{\qp(\zz|\xx,\yy)}{\qp(\zz)} \Big] - \kldiv\Big[ q(\XX) \| \ptgreen(\XX) \Big] + \\
& \ \ \ \ -\kldiv \Big[ \qp(\ZZ) \| p(\ZZ|\YY) \Big] + \mathbb{E}_{q(\yy)} \log p(\yy). \tag{13c} \\
\end{align}

Here, we need to choose what $p(\zz|\yy)$ is. Either it can be a fixed distribution (i.e. a distribution is pre-assigned for each possible value of $\yy$), or it could also be /learned/, in which case we can denote it as $\ptgreen(\zz|\yy)$.

# It is useful to note that there are two ways in which the joint distribution for a VAE can be expressed, and these come down to the independence assumptions on $X, Y, Z$.  
# If we assume that the ground truth $p(\yy, \zz) = p(\zz)p(\yy)$ 
# Here, the KL term is between $\qp(\zz|\xx,\yy)$ and $\pgreen(z)$, and $\pgreen(y)$ falls out as one of the constants. Despite this, $\pgreen(\yy)$ can take on one of two interpretations: either it is a prior that we set just like $\pgreen(\zz)$, or it is the empirical distribution over $\yy$'s 
# For this post we will assume an independent conditional structure, which means we assume $Z$ and $Y$ are independent. This is a useful assumption to make if we wish to optimise a variational autoencoder where those variables are disentangled and encode semantically different things. For instance, if $Y$ is some semantic label of $X$ (e.g. images of dogs in the wild) then we could think of $Y$ as encoding exactly that and $Z$ encoding sources of stochasticity such as background details and other things not related to dogs. The issues that I talk about here are still relevant to entangled VAEs, because the fundamental issue I want to speak about is that which involves training a VAE that is modelling the effect of two latent variables.

* References

- =beckham2023thesis= Beckham, C. (2023). PhD thesis dissertation. (Work in progress.)
- =kingma2013auto= Kingma, D. P., Welling, M., & others, (2019). An introduction to variational autoencoders. Foundations and Trends in Machine Learning, 12(4), 307â392.
- =kingma2019introduction= Kingma, D. P., Welling, M., & others, (2019). An introduction to variational autoencoders. Foundations and Trends in Machine Learning, 12(4), 307â392.
- =esmaeili2018structured= Esmaeili, B., Wu, H., Jain, S., Bozkurt, A., Siddharth, N., Paige, B., Brooks, D. H., â¦ (2018). Structured disentangled representations. arXiv preprint arXiv:1804.02086, (), . TODO fix citation
- =burgess2018understanding= Burgess, C. P., Higgins, I., Pal, A., Matthey, L., Watters, N., Desjardins, G., & Lerchner, A. (2018). Understanding disentangling in beta-VAE. arXiv preprint arXiv:1804.03599, (), .
- =child2020very= Child, R. (2020). Very deep VAEs generalize autoregressive models and can outperform them on images. International Conference on Learning Representations, (), .
- =ho2020diffusion= Ho, J., Jain, A., & Abbeel, P. (2020). Denoising diffusion  probabilistic models. Advances in Neural Information Processing Systems, 33(), 6840â6851.
- =dhariwal2021diffusion= Dhariwal, P., & Nichol, A. (2021). Diffusion models beat GANs on
  image synthesis. Advances in Neural Information Processing Systems,
  34(), 8780â8794.
