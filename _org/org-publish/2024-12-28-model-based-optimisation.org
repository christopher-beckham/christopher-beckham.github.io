#+OPTIONS: toc:nil
#+LATEX_HEADER: \newcommand{\ft}{f_{\theta}}
#+LATEX_HEADER: \newcommand{\ftrain}{f_{\text{train}}}
#+LATEX_HEADER: \newcommand{\fvalid}{f_{\text{valid}}}
#+LATEX_HEADER: \newcommand{\ftest}{f_{\text{test}}}
#+LATEX_HEADER: \newcommand{\fphi}{f_{\phi}}
#+LATEX_HEADER: \newcommand{\ds}{\mathcal{D}}
#+LATEX_HEADER: \newcommand{\pt}{p_{\theta}}
#+LATEX_HEADER: \newcommand{\ptnew}{\widehat{p_{\theta}}}
#+LATEX_HEADER: \newcommand{\ptrain}{p_{\text{train}}}
#+LATEX_HEADER: \newcommand{\pvalid}{p_{\text{valid}}}
#+LATEX_HEADER: \newcommand{\dtrain}{\mathcal{D}_{\text{train}}}
#+LATEX_HEADER: \newcommand{\dvalid}{\mathcal{D}_{\text{valid}}}
#+LATEX_HEADER: \newcommand{\dtest}{\mathcal{D}_{\text{test}}}
#+LATEX_HEADER: \newcommand{\drest}{\mathcal{D}_{\text{rest}}}
#+LATEX_HEADER: \newcommand{\argmax}{\text{argmax}}
#+LATEX_HEADER: \usepackage{tcolorbox}
#+bibliography: mbo.bib
#+cite_export: csl ieee.csl

#+BEGIN_EXPORT html
---
title: bridging offline and online model-based optimisation
description: TODO
layout: default_latex
---

<h1>bridging offline and online model-based optimisation</h1>

<div hidden>
<!-- This should be consistent with LATEX_HEADER -->
$$\newcommand{\argmax}{\text{argmax}}$$
$$\newcommand{\ft}{f_{\theta}}$$
$$\newcommand{\ftrain}{f_{\text{train}}}$$
$$\newcommand{\fvalid}{f_{\text{valid}}}$$
$$\newcommand{\ftest}{f_{\text{test}}}$$
$$\newcommand{\fphi}{f_{\phi}}$$
$$\newcommand{\ftt}{f_{\theta}}$$
$$\newcommand{\ds}{\mathcal{D}}$$
$$\newcommand{\pt}{p_{\theta}}$$
$$\newcommand{\ptnew}{\widehat{p_{\theta}}}$$
$$\newcommand{\ptrain}{p_\text{train}}$$
$$\newcommand{\pvalid}{p_\text{valid}}$$
$$\newcommand{\dtrain}{\mathcal{D}_{\text{train}}}$$
$$\newcommand{\dvalid}{\mathcal{D}_{\text{valid}}}$$
$$\newcommand{\dtest}{\mathcal{D}_{\text{test}}}$$
$$\newcommand{\drest}{\mathcal{D}_{\text{rest}}}$$
</div>

#+END_EXPORT

#+BEGIN_EXPORT html
<div id="images">
<br />
<figure>
<img class="figg" src="/assets/mbo/mbo-header.png" width="600" alt="" />
</figure>
<figcaption><b><i>(attribution: cite paper)</i></b></figcaption>
<br />
</div>
#+END_EXPORT

# Some bullshit to be aware of:
# - org-cite-insert doesn't like enter, you have to do C-M-j 
#   - See https://www.reddit.com/r/orgmode/comments/q58f4f/how_to_actually_insert_a_citation_with_orgcite/

#+TOC: headlines 3

# In this blog post, I give a brief introduction to model-based optimisation, explain a fundamental research question I tried to pursue last year in the context of /offline/ model-based optimsiation (one half of the problem), and then reflect on that work and how it relates to /online/ (the other half of the problem).

In this post, I introduce offline model-based optimization and share highlights from research I published at the tail end of my PhD. The focus is on issues of validation and extrapolation -- two topics that remain frustratingly underexplored in the field. Along the way, I offer an honest critique of my own work, reflect on broader challenges in offline MBO, and point to a few promising directions forward.

While I’m no longer working full-time in academia, this topic still matters a lot to me. If any of it resonates with you -- whether you’re a researcher, engineer, or just curious, I’d love to chat (albeit in a bandwidth-conscious capacity).

* intro - what is model-based optimisation?
:PROPERTIES:
:CUSTOM_ID: sec:intro
:END:


# context: MBO, we want to design inputs, ones which maximise some desiderata which is encoded by a real world reward function.
In model-based optimization (MBO), the goal is to design inputs—often tangible, real-world objects—that maximize a reward function reflecting some desired property. Crucially, this reward function exists in the real world, meaning that evaluating it requires physically testing the input and observing its outcome. For example, in protein design, the input might be a molecular specification, and the reward could be the protein’s ability to bind to a receptor. In this setting, the true objective $f(x) \in \mathbb{R}^{+}$ measures binding affinity, and querying it involves running expensive lab experiments. We will refer to this "true" objective function as the /reward oracle/.

# online: use the ground truth to guide the search, active labelling
# however, this is expensive
In theory, we could optimise the reward oracle $f$ directly by combining it with a search algorithm, learning from past evaluations to guide the search for better inputs. This involves learning a "surrogate" model $\ft$, which is exploited by the search algorithm and also refined iteratively based on feedback provided by the reward oracle function. This is called /online model-based optimisation/ ("online MBO"). In practice however, querying $f$ is often prohibitively expensive and slow, and we need many samples if we want the model (the surrogate) to learn enough about the underlying problem in order to guide the search towards promising candidates. 

But if we already have an existing dataset of input-reward pairs ($y = f(x)$ is the "reward"), we can use it to /pre-train/ the surrogate model. The idea is that a well-trained surrogate may then require far fewer queries during the online phase to guide the search effectively. This pre-training stage is known as /offline model-based optimisation/ ("offline MBO").

# conclusion: proxy is difficult, mbo is difficult
Since $\ft$ is an approximation however, it is particularly brittle. It can behave unpredictably on regions of input space which are not represented in the training data ("out-of-distribution"), and this is especially problematic since we assume the best input candidates are not already yet in the training set. It is also vulernable to adversarial examples, where by naively optimising for the highest-scoring candidate with respect to the surrogate often produces a candidate which is no longer plausible according to real world constraints. This makes it an interesting yet challenging area of research.

# MBO can be categorised into two varieties, online and offline. In online, we assume that $f$ /can/ be queried during training. One such instance is Bayesian optimisation applied to this setting: we have a GP regression model $\ft$ and the learning algorithm alternates between proposing candidates $x$ (via some search algorithm) and subsequently invoking the ground truth $y = f(x)$. From this, we can treat $(x,y)$ as a newly acquired data point to incrementally update $\ft$ and the process continues.

# Assuming $\ft$ is "expressive" enough and it is economically viable to obtain "enough" samples from $\ft$ (which isn't practical, but more on this later), then surely we can learn a good model.

** formalising offline mbo
:PROPERTIES:
:CUSTOM_ID: sec:intro_whatis
:END:

# context: this is the math describing offline mbo, also we seg into bayes rule
In the offline setting, we assume access to a labelled dataset of input-output pairs, denoted $\mathcal{D} = \{(x_i,y_i)\}_{i=1}^{N}$, where $y_i \in \mathbb{R}^{+}$. The goal is to learn a model of the reward -- typically a surrogate function $\ft$ -- without querying the true function during training or model selection. While much of the offline literature focuses on a surrogate $\ft$, it's arguably more useful to think in terms of a generative model that defines a joint distribution $\pt(x,y)$. Via Bayes' rule, it can factorise in one of two ways.

# content: first factorisation
The first is $\pt(y|x)p(x)$, and we can think of $\pt(y|x)$ as the probabilistic form of the surrogate, i.e. $\ft(x)$ parameterises the mean of the distribution. We can interpret this as: first we sample $x$ from some prior (e.g. the training distribution), then we predict its reward. While this is /technically/ a generative model, it is not particularly useful for sampling high-scoring candidates as the only sampling we do is from the training data (and not a model we have learned).

# content: first factorisation, doesn't make much sense
In practice, a common baseline takes a hybrid approach which doesn't quite correspond cleanly to this. This involves sampling $x$ from the training data, which is then iteratively updated by ascending the gradient of $\ft(x)$ (typically the mean of $\pt(y|x)$). While this produces inputs with higher predicted reward, it abandons the semantics of the above factorisation and tends to produce poor inputs when scored against the reward oracle. (While online MBO also does a sort of hill climbing on the surrogate, the difference is that the resulting input is validated against the reward oracle, and this data is used to update the model.)

# content: second factorisation, also it makes more sense
# also conclusion.
The second factorisation is $\pt(x|y)p(y)$, which we can think of as saying: first choose the desired reward $y$, then find an input which has that reward. Since $\pt(x|y)$ is a /conditional generative model/, not only can we target high reward regions, but we can also avoid generating implausible inputs since it is a mechanism built into the model. (While generative models are not by any means invulernable to generating such inputs, the key idea is that plausibility to built into the model by design.)

For the remainder of this work, we will define our joint generative model $\pt(x,y)$ as the second factorisation:

\begin{align}
\pt(x,y) = \pt(x|y)\ptrain(y),
\end{align}

where $\ptrain(y)$ is the empirical distribution over the rewards in the training set, and $\pt$ is also learned from this.

# This framing aligns naturally with /generative models/, which are designed to model the distribution of the data directly. Furthermore, since this is a conditional generative model, we get to have a model which can target both high-reward regions and also avoid generating unrealistic or adversarial inputs. 
# conclusion: 2nd factorisation makes more sense, and generative models fit the task.
# In the offline MBO setting, this is especially appealing. Since it is too expensive to interact with the ground truth reward function during training, we want a model which can both target high-reward regions and avoids generating unrealistic or adversarial inputs. Conditional generative models $\pt(x|y)$ offer a principled and practical way to achieve this. While generative models are not by any means invulernable to generating adversarial or implausible inputs, the key point is that plausibility is built into the model by design.

** ‼️ reward-based extrapolation
:PROPERTIES:
:CUSTOM_ID: sec:intro_extrapolate
:END:

# context: we don't just want to generate, we want to extrapolate, but how do we do this
The key idea which seperates MBO from regular generative modelling is that we don't just want to generate any kind of sample from the model. We would like to generate samples whose /real/ reward $y$ is as large as possible, as these have the most real world utility. The difficulty lies in the fact that these (extremely) high scoring samples do not exist in the training set, otherwise MBO would not be needed to begin with. Furthermore, it means models have to /extrapolate/ -- the model cannot see high scoring samples in the training set in order to produce similar things. Rather, it has to learn what constitutes low and medium-scoring samples, and infer what a high-scoring sample may look like.

# content: explain that we need to change the prior
This also implies that the behaviour of the generative model needs to somehow be 'tweaked' at generation time. For instance, we have defined generative model $\pt(x,y)$ to be the following:

\begin{align}
\pt(x,y) = \pt(x|y)\ptrain(y),
\end{align}

where $\ptrain$ is the empirical distribution of $y$'s observed in training. If we simply sample according to this strategy, we will only sample conditioned on the kinds of reward seen in the training set. To rectify this, we /could/ switch out the prior for another distribution $\widehat{p}(y)$, one which reflects a larger distribution of rewards. For instance, if $\ptrain(y)$ reflects a range of values from $[0,100)$, perhaps the new prior reflects those from $[100,200]$. From this, we can define the "extrapolated" model:

\begin{align}
\widehat{\pt}(x,y) = \pt(x|y)\widehat{p}(y).
\end{align}

(I am the widehat notation '$\widehat{\pt}$' to symbolise 'higher', a version of $\pt$ which is biased towards high scoring samples, rather than something implying a statistical approximation.)

Ideally we would like to find an "extrapolated" model $\widehat{\pt}(x,y)$ such that it maxmises:

\begin{align}
m_{\text{test}}(p; f)|_{p=\widehat{\pt}} = \mathbb{E}_{x \sim \widehat{\pt}(x,y)} f(x). \tag{1}
\end{align}

In other words, we want to find a $\pt(x|y)$ and $\widehat{p}(y)$ such that samples produced by the former have as large of a reward as possible, according to the reward oracle. Since this equation involves $f$ which is too expensive to compute during training or model selection, it is only intended to be executed at the very end of the machine learning pipeline. This means that evaluating Eqn. (1) during those stages is not feasible. 

To rectify this, we could simply replace $f$ with the surrogate model $\ft$. However, $\ft$ has also only been trained on the same empirical distribution of rewards, and we cannot expect it to score inputs conditioned on e.g. $[100,200]$ reliably, as this is clearly out-of-distribution. One approach is to split the dataset into low-to-moderate scoring examples and high-scoring examples. For instance, if our /original/ dataset only represented samples with reward in $[0,100]$, then we could for instance split it into $[0,50]$ (low-to-moderate) and $[50,100]$ for high scoring (see Fig. 1). The low-to-moderate split is used to train the generative model, while the latter forms a validation set. Both data splits (which is just the full dataset) is used to train a /validation oracle/, $\fvalid$.

To evaluate the generative model's ability to extrapolate, we simply run its "extrapolated" variant (sample from $\widehat{p}(y)$), and the corresponding samples can be effectively validated by the validation oracle precisely because it already saw those range of rewards during training. Therefore, this setup allows us to measure not just generalization, but generalization specifically in the context of /reward extrapolation./


#+BEGIN_EXPORT html
<div id="images">
<br />
<figure>
<img class="figg" src="/assets/mbo/mbo-train-val-workflow.png" width="700" alt="" />
</figure>
<figcaption><b><i>Figure 1: "Actual data distribution" signifies the real data, where the assumed max reward is 200. Since we assume our offline dataset only contained reward values between 0 and 100, if we want to measure extrapolation then we need to split this into two subsets. In this illustration, 0-50 is chosen for train and 50-100 for validation.</i></b></figcaption>
<br />
</div>
#+END_EXPORT

** ‼️ why evaluation is difficult (and misunderstood)
:PROPERTIES:
:CUSTOM_ID: sec:intro_evaldifficult
:END:

# context: shift in generative modelling -> need to rethink eval
With the rapid progress in generative modeling over the past few years, our approach to evaluation has evolved. In earlier eras of machine learning, it was common to assess models based on likelihood over a test or validation set -- a natural outcome of maximum likelihood estimation, where the goal is to find parameters $\theta$ that maximise the probability of the observed data.

# content (details on likelihood vs sample based eval, how surrogates fit in).
Because of the extremely rapid advances in generative modelling in the past few years, the way we have performed evaluation has changed. In the olden days of machine learning, it was more common to evaluate machine learning models by way of likelihood on a test or validation set. This is a natural consequence of maximum likelihood estimation, which states that we wish to find a model which best "explains" the data, i.e. find parameters $\theta$ such that the parameterised model assigns the highest average likelihood across all samples. However, likelihood is only concerned with how plausible /pre-collected samples/ are, rather than whether samples generated from the model itself satisfy a useful notion of preference. (Also, likelihood isn't a particularly accurate measure of sample quality. [cite:@huszar2015not;@theis2015note;@ttur]) Such preferences can be encoded with a reward function $f$, but this is typically expensive to compute as it reflects a real world process (i.e. $y = f(x)$ is like asking a human rater to evaluate $x$).

# conclusion: validation is hard and underexplored.
As mentioned in Sec. [[#sec:intro_extrapolate]], a principled strategy is to approximate $f$ with $\fvalid$ and continue forward. Even if $\fvalid$ is an approximation, it actually serves as a useful anchor for the generative model. This is because even though it is only trained on low-to-moderate scoring inputs, we can measure its ability to generate high-scoring inputs against the validation oracle which has technically seen high scoring inputs during training. Compared to other MBO literature, I make a very explicit distinction between /validation/ and /testing/ which does not seem to be well-respected in offline MBO literature, and I partly suspect it's because there is a conflation between /"real world" MBO/ and /"academic" MBO/. (These are terms I made up, so don't expect to find them anywhere else.)

By "academic MBO" I simply mean doing MBO in the context of academic research, i.e. publishing papers. In this situation it may not be practically feasible to evaluate the reward oracle $f$, for instance in the case where the benchmark data involves an extremely expensive human evaluation (e.g. protein synthesis). To rectify this, some MBO datasets are actually based on simulation environments, where datasets are examples extracted from the simulator. By extension, they also provide /simulated oracles/ which can also be treated as if they were reward oracles.[fn:sim2real]

[fn:sim2real] This should not be interpreted as discouraging "sim2real" experiments, where simulators are used to pre-train a model which is then adapted to a real world task. The difference is that ...


Since the simulator is just a function that can be freely executed /in silico/ with negligible monetary cost, researchers can (intentionally or not) violate the spirit of offline MBO by "abusing" the simulator and constantly testing it against whatever model is being trained. This is especially enticing in academia because there is an overwhelming bias towards pushing things that "beat SOTA" or are "novel". Conversely, in "real world" MBO there is already a safeguard against abusing the ground truth and that is money. Therefore, in order to be economical, a validation set needs to set aside to validate experiments before any generated samples get sent off to the real world.

# We will elaborate on this in the next section.

#+BEGIN_EXPORT html
<div id="images">
<br />
<figure>
<img class="figg" src="/assets/mbo/academic_vs_real_mbo.png" width="800" alt="" />
</figure>
<figcaption><b><i>Figure 2: In "academic MBO", what is meant to be treated as an expensive-to-evaluate reward oracle is not treated as such, since it doesn't truly represent a real world process. It can either take the form of a simulation environment (which is significantly cheaper to compute than a real world process), or a neural approximation trained on held-out data (e.g. a test set), which is also cheap to compute. Conversely, in "real world" MBO, the ground truth is truly too expensive to compute for training  or model selection, so a validation set is needed.</i></b></figcaption>
<br />
</div>
#+END_EXPORT


Due to the different types of "ground truth" oracle shown in Fig X, it will be useful to standardise some phrases. 

| name                                 | what is it                                                                                                              |
|--------------------------------------+-------------------------------------------------------------------------------------------------------------------------|
| reward oracle                        | $f(x)$: real world process, extremely expensive to compute                                                              |
| simulated oracle                     | $f(x)$: emulates the real world (somewhat cheap to compute). Here we will call it the **test oracle**                   |
| proxy oracle                         | -  $\ft$: model based on training set, samples are created with this model, also called a "surrogate" (I prefer to use $\pt(x \vert y)$ here) |
|                                      | - $\fvalid$ : proxy oracle trained on train set + valid test. Here we will call it the **validation oracle**            |
|                                      | - $\ftest$ : proxy oracle trained on train + valid + test set (all of the data), exists when reward or simulated oracle do not exist. Here we will call it the **test oracle** |

TODO write about sim2real

# From now on, "ground truth reward" will refer to either a real-world MBO setting or a simulation environment. Conversely, a "test surrogate" (analogous to a validation surrogate) is an /approximate model/ (i.e. a neural network trained on some data) which is intended to be treated as the ground truth.

# reward oracle
# simulated oracle
# proxy oracle -> [validation oracle, test oracle]

** the train/val/test recipe, moving forward
:PROPERTIES:
:CUSTOM_ID: sec:intro_summary
:END:

As discussed in Sec. [[#sec:intro_extrapolate]], we need to measure not just generalisation, but extrapolation. If our validation set follows the proposed setup in Fig. (1), then we can just approximate Eqn. (1) by using $\fvalid$ in place of $f$, as well as $\pt(x,y)$ in place of $\widehat{\pt}(x,y)$:

\begin{align}
m_{\text{test}}(\widehat{\pt}; f)_{|f = \fvalid, \widehat{p}(y)=\pvalid(y)} &= \mathbb{E}_{x \sim \widehat{\pt}(x,y)} \fvalid(x) \\
& = \mathbb{E}_{x \sim \widehat{\pt}(x|y)\pvalid(y)} \fvalid(x). \tag{2}
\end{align}

which constitutes our first validation metric. By "validation metric" I simply mean some function which measures the "goodness" of the generative model $\pt$ using either the validation set, its corresponding reward oracle, or both. (We will discuss some other ones later.)

<<ref-fvalid-concern>> Note that while Eqn. (2) is a principled and reasonable approach to determining how well $\pt(x|y)$ extrapolates, this is just one possible validation metric of many. On one hand, it is quite interpretable: assuming a fixed $\fvalid$, Eqn. (2) is maximised when samples produced from $x \sim \pt(x|y), y \sim \pvalid(y)$ produce the largest average reward. On the other hand, $\fvalid$ is an approximate model and shares the same vulnerabilities to adversarial examples and overconfidence as many other regression model. Therefore, validation metrics go beyond just simply directly approximating Eqn. (1), and may involve measuring other aspects of the generative model or data.

# How validation sets are handled in existing offline MBO literature is unfortunately not clear. As an example, some existing works   [cite:@fannjiang2020autofocused;@brookes2019conditioning;@mins] make use of a training split as well as an approximate "test surrogate" model trained on the full dataset, but as mentioned, if this is used in model selection then it violates the spirit of offline MBO. As such, I suggest the following strategy, even though it appears to be "non-standard". Given our dataset $\mathcal{D}$:

So far we have discussed the need to measure extrapolation (Sec [[#sec:intro_extrapolate]]), as well as the lack of a validation set which is crucial to measuring it (Sec [[#sec:intro_evaldifficult]]). From this we can motivate a very principled and reasonable train-validate-test recipe, which is the following:

- *Inputs*: From the entire dataset $\mathcal{D}$, split them into: $\dtrain$, $\dvalid$, and $\dtest$. Also ensure that the valid and test sets contain higher reward inputs, as per Sec. [[#sec:intro_extrapolate]].
- *Training*: Train the generative model $\pt(x|y)$ on $\dtrain$. Also, if needed, train a /validation oracle/ $\fvalid$ on $\dtrain \cup \dvalid$.
- *Validation*: Use $\dvalid$ and/or $\fvalid$ for model selection / hyperparameter tuning.
- *Final evaluation*: given a recipe for generating high scoring samples from the model, score with either the reward oracle ("real world MBO"), or simulator or approximate test oracle ("academic MBO").

*Finally*: for "real world MBO" step in "final evaluation", since you'll be sending off samples to the real world, you could also maximise your chances of success by re-training $\pt(x|y)$ on all of the data, then use that.

In the absence of a reward oracle which can be judiciously evaluated, we need to turn to validation metrics. How can we find cheap-to-compute validation metrics to help us search for good models?


#+BEGIN_COMMENT
\paragraph{Use of validation set} Compared to other works, the use of a validation set varies and sometimes details surrounding how the data is split is opaque. For example, in \cite{mins} there is no mention of a training or validation set; rather, we assume that only $\dtrain$ and $\dtest$ exists, with the generative model being trained on the former and test oracle on the latter (note that if the test oracle is approximate there is no need for a $\dtest$). This also appears to be the case for \cite{fannjiang2020autofocused}. While Design Bench was proposed to standardise evaluation, its API does not prescribe a validation set\footnote{However, in \cite{trabucco2022designbench} (their Appendix F) some examples are given as to what validation metrics could be used.}. While the training set could in principle be subsetted into a smaller training set and a validation set (such as in \cite{qi2022data}), the latter would no longer carry the same semantic meaning as \emph{our notion} of a validation set, which is intentionally designed to \emph{not be} from the same distribution as the training set. Instead, our evaluation framework code accesses the \emph{full} dataset via an internal method call to Design Bench, and we construct our own validation set from it. We illustrate these differences in Figure \ref{fig:mbo_data_splits}.
#+END_COMMENT


[fn:1] The validation metric and test metric here cannot be the same, since the latter relies on expensive-to-compute $f$. This issue can also be seen in other domains, for instance in LLMs the validation metric is a cheap to compute proxy like BLEU score, while the test metric involves human feedback.

[fn:2] Technically, the test metric (Eqn. (1)) could just be a function of a "test surrogate" model $\ftest$ (for instance, if the data is cut up into train / valid / test, train $\fvalid$ on {train,valid} and train $\ftest$ on {train,valid,test}, however now we have to accept that there is a degree of uncertainty involved with the test metric as well.

[fn:3] A similar thing happens in reinforcement learning.


The types of datasets
- (1) Simulations of real-world phenomena, for instance reinforcement learning environments. In [cite], some examples involve optimising for robot morphologies which are then used with a pre-specified policy to measure how far it can run.
- (2) Real-world phenomena, e.g. superconductors, but the ground truth comes from the real world and so the best can do is use a test surrogate $\ftest$.
- (3) Synthetic functions (e.g. see X). These functions are commonly used to test optimisation algorithms, however these are well-supported within the input space or a large hypercube and can make it difficult for generative models to learn any structure in the data.

# In the case of (1), we do have accessible and cheap to compute ground truth. Furthernmor

While (2) is most representative of a real world MBO problem, we can exploit the "in silico" datasets of (1) and take advantage of the fact that the ground truth is easily available. This motivated the work I published where I wanted to devise a principled method for finding validation metrics which are highly-correlated with the ground truth. If we could find such metrics, then we could use them in real world MBO pipelines where the ground truth isn't easily available.

* last year's work
:PROPERTIES:
:CUSTOM_ID: sec:last_year
:END:

Let us begin with a summary of everything so far:

- (1) In offline model-based optimisation we wish to learn a reward-conditioned generative model from an offline dataset of input-reward pairs. The rewards are originally obtained from a ground truth reward "oracle", which is assumed to be too expensive to query during training or validation of the generative model.
- (2) Evaluating samples from a generative model is a /difficult/ task. Firstly, likelihood-based evaluation is not sufficient to evaluate the quality of outputs. Secondly, samples ideally need to be evaluated by human feedback, and is neatly capturable by the notion of a reward oracle. Lastly, models trained need to /extrapolate/ beyond the rewards they were trained on, as the better they can extrapolate, the more impactful they will be in the real-world.
- (3) Evaluation is difficult, often neglecting a validation set. This may be related to the confusion between "real world" and "academic" MBO. In "academic MBO", the reward oracle is replaced with a simulator or approximate test oracle. While these are technically useful and cheap-to-compute, non-sparing use of these fundamentally violate the /spirit/ of offline MBO, whose emphasis is on trying to extract as much value as possible from the available data without resorting to expensive reward oracle queries.
- (4) In the absence of a reward oracle which can be judiciously evaluated, we need to turn to validation metrics, which are assumed to be cheap to compute. How can we such metrics to help us build better generative models?

The work I published last year addresses these bullet points of Sec. [[#sec:intro_summary]]. 
# Firstly, we want our models to /extrapolate/. Secondly, evaluation doesn't follow rigorous practice and seems to conflate real world and academic MBO. I proposed a way to address both of these issues at once, which is simply to use a validation set (as well as a validation surrogate), and also to ensure the validation set contains a larger range of rewards than that of the training set.

To implement the train-valid-test protocol described in X, some technical considerations were needed. Experiments were implemented with /Design Bench/, a popular MBO benchmarking framework [cite:@trabucco2022designbench]. Design Bench imposes a reward threshold $\gamma$ which dictates which samples are assigned to the training set. For example, any samples whose $y \leq \gamma$ are assigned to the training set, otherwise they are hidden from the user. Because of this, all of the remaining samples $\gt \gamma$ are not assigned to a validation set -- in fact, the library does not prescribe one at all. Two possible solutions are:

- (1) Simply hold out some small part of the training set as the validation set. This respects the intended design of the library, but effectively reduces the size of the training set and may make it difficult for trained models to be competitive with the literature. (In Fig. 1 left, $\dtrain$ is shown here, so imagine cutting out some portion of this as the validation set.)
- (2) Define that all samples whose $y \gt \gamma$ belong to the validation set (Fig. 1, right). Since the validation surrogate $\fvalid$ is always trained on the combined train+valid split, this means it is trained on the full dataset. This technique does not respect the intended design of the library, even if its motivation is quite principled.

# Since that can be a bit difficult to visualise, it is illustrated in Fig. (3).

I ended up choosing (2). However, this requires some nuance when it comes to interpreting the relationship between the validation surrogate $\fvalid$ and the test oracle $\ftest$. If the dataset defines $\ftest$ as a simulator, then we can train $\fvalid$ on the full dataset as the simulator can be treated as the ground truth from which the dataset's samples were drawn from.

#+BEGIN_EXPORT html
<div id="images">
<br />
<figure>
<img class="figg" src="/assets/mbo/split1.png" width="350" alt="" /> &nbsp; &nbsp; <img class="figg" src="/assets/mbo/split2.png" width="350" alt="" /> 
</figure>
<figcaption><b>Figure 3.</b> <i>Left:</i> Design Bench's API exposes a training set which is all samples which fall below the threshold gamma (blue). <i>Right:</i> By considering samples which exceed gamma, we can define a validation set consistent with Sec. 1.3. The validation oracle is then trained on both the validation and training splits, which effectively is the full dataset.</figcaption>
<br />
</div>
#+END_EXPORT


Otherwise, if $\ftest$ is actually an approximate test oracle, then by Design Bench's definition it has been trained on the full dataset. This means training a validation surrogate would in turn involve training on all of the data and therefore be equivalent to a test oracle, and the latter needs to have seen more data to be a useful tool to measure generalisation at the very end. Therefore, we let the test surrogate remain as the "gold standard" which has been trained on all of the data, and we only allow the validation surrogate to be trained on a subset of the full dataset. Concretely, this would be the training set, plus a 50% subsample of any examples whose reward exceeds $\gamma$.

#+BEGIN_EXPORT html
<div id="images">
<br />
<figure>
<img class="figg" src="/assets/mbo/split3.png" width="350" alt="" /> &nbsp; &nbsp; <img class="figg" src="/assets/mbo/split4.png" width="350" alt="" /> 
</figure>
<figcaption><b>Figure 4.</b> Design Bench defines the test proxy as being trained on the full dataset, but the validation proxy has also been defined in the same way (<i>left</i>). To resolve this, we can define the validation set as being a random subsample of samples greater than gamma (in the figure, this is 50%). That way, the test oracle still gets to be trained and defined with respect to the full dataset and serve its purpose as a gold standard to measure generalisation.</figcaption>
<br />
</div>
#+END_EXPORT

# In my work I decided with (2). We now finally have a validation set! The only thing that is left is to define the validation metrics. From this, a reasonable train/val/test pipeline would be:

- **Training**: train $\pt(x,y) = \pt(x|y)\ptrain(y)$ on $\dtrain$, where $\ptrain(y)$ is the empirical distribution over $y$'s for the training set.
- <<ref-bullet-validation>> **Validation**: Switch out $\ptrain(y)$ for $\pvalid(y)$, which defines a new generative model $\ptnew(x,y)$. Use this in conjunction with a validation metric. We will define a few of these later, but we may also assume that any of these metrics may /also/ be a function of a validation surrogate.
- **Test**: once the best $\ptnew$ is determined according to the validation metric, finally score the model on the real ground truth by invoking Eqn. (1). For "real world MBO", this is the ground truth $f(x)$, for "academic MBO" this is the "test surrogate", $\ftest(x)$.

** ranking validation metrics
  :PROPERTIES:
  :CUSTOM_ID: sec:last_year_valid_metrics
  :END:

Now, all that is left is a validation metric. All I mean by this is something we can use to measure how well the generative model performs. This metric is a function of the generative model $\pt$, and may also be a function of the validation proxy $\fvalid$ and validation set $\dvalid$.  We already saw one of these metrics, which is simply Eqn. (1) but with $\fvalid$ substituted for $f$:

\begin{align}
m_{\text{test}}(p; f)_{|f = \fvalid, p=\ptnew} & = \mathbb{E}_{x \sim  \ptnew(x,y)} \fvalid(x) \tag{3} \\
&= m_{\text{reward}}(p; \fvalid),
\end{align}

and this is a function of just the generative model, the validation surrogate, and also /part/ of the validation set (the empirical distribution over $y$). This metric doesn't particularly care about how "calibrated" the model is. For instance, if we condition on $y = 50$ and get an example whose reward according to $\fvalid$ is $1000$, the model doesn't get penalised for it. The only thing that matters is that the samples from $\ptnew$ score as high as possible on average. Otherwise, if this is this is concerning, another validation metric is the "agreement" [cite:@mins], which measures the extent to which the validation surrogate agrees with the supposed label of the input generated by the model:

$$m_{\text{agreement}}(p; f)_{|f=\fvalid,p=\ptnew} = \mathbb{E}_{x \sim \ptnew(x,y)} (y - \fvalid(x))^2. \tag{4}$$ 

For example, if we sample $y=50$ to generate an example and this is what $\fvalid$ also agrees with it and predicts the same value, then the resulting loss will be zero. If $\hat{p}(y)$  comes from the validation set (i.e. $\hat{p}(y) = \pvalid(y)$), then from the point of view of the generative model these will be high scoring samples, so this metric also selects for high scoring candidates. But the additional constraint here is that the validation proxy must also /agree/ with the generative model.

In principle, these metrics could be combined together as a sum, but this slightly complicates the analysis as we also have to determine suitable scaling factors for each term.

# https://chatgpt.com/c/67a77ee2-5fbc-8008-b434-62a547cfed98

Other validation metrics I defined were:

- $\mathcal{M}_{\text{FD}}$: Frechet Distance (/FD/) ([cite:@dowson1982frechet; @ttur]) between the distribution of samples coming from $\ptnew$  and the validation set. Note that this is /not/ the same as Frechet /Inception/ Distance (/FID/), which uses the ImageNet-pretrained Inception network as a feature extractor. Here, we can define the feature extractor as being some suitable bottleneck in $\fvalid$.
- $\mathcal{M}_{\text{DC}}$: The "density and coverage" metric proposed in [cite:@kynkaanniemi2019improved], which is an improved version of the precision and recall metric originally proposed in [cite:@sajjadi2018assessing]. This metric was originally motivated to tease out two important factors which determine how close two distributions are: sample quality and mode coverage, which can be thought of as precision and recall, respectively. While these terms can be individually computed, here I simply sum both terms, simply treating it as an alternative metric which can be compared to FD.
- $\mathcal{M}_{\text{C-DSM}}$: The conditional denoising diffusion loss [cite:@ho2020denoising] but evaluated on the validation set. Essentially, we are asking how well the model can denoise high scoring samples that it has never seen before. Since DDPMs are /likelihood-based/ models, this is also a likelihood-based loss and therefore may not correlate well with sample quality. However, it is trivial to incorporate as a validation metric since it is already defined as a training loss.
- $\mathcal{M}_{\text{reward}}$, this is just Eqn. (1) and we use $\ftest$.

Going back to the purpose of this work, we ask: what validation metrics work best, and how do we measure it? Ideally, evaluating validation metrics requires access to the oracle, as we need to measure them up against some gold standard. That’s where simulation environments become interesting: they give us access to a something which very closely mimics a real world oracle, letting us test how well different validation metrics correlate with the actual ground truth. The idea is to use this setup to run a large-scale comparison of metrics across many simulated datasets, so we can better understand which validation metrics are most trustworthy when we don’t have access to the ground truth. Ideally, this gives us actionable guidance for real-world MBO deployments.

So how do we use the ground-truth function $f$ to evaluate validation metrics? In this work, I used /denoising diffusion probabilistic models/ (DDPMs) [cite:@ho2020denoising], due to their flexibility and strong performance in generative modeling. Using a fixed DDPM architecture, I varied several hyperparameters, such as network width, reward dropout probability[fn:ddpm], and reward guidance strength. Each unique hyperparameter setting defines a /configuration/, and for each configuration, a model was trained via early stopping on a validation metric. (So roughly a total of $N_{\text{hyperparams}} \times N_{\text{datasets}} \times N_{\text{metrics}}$ models.) 

From each of those configurations' checkpoints, samples were produced via $\ptnew(x,y)$, and the reward oracle was used to predict the mean reward. This mean reward was plotted against the validation metric. This means that for each validation metric $\mathcal{M}_j$, we obtain a scatterplot of the mean reward $\mathcal{M}_{\text{reward}}$ versus $\mathcal{M}_j$, and from this the Pearson correlation can be computed.


[fn:ddpm] Conditional diffusion models are typically trained with dropout on the conditioning variable $y$ (in our case, the reward). This makes them act as both unconditional and conditional models.


These results are illustrated below for Design-Bench’s continuously-valued datasets.[fn:cont] In particular, Ant, Kitty, and Hopper are simulated environments with exact oracles, making them ideal for this type of analysis. For completeness, I also include Superconductor, which uses a non-exact oracle but still provides a useful point of comparison.

[fn:cont] Continuous datasets were used as DDPM operates on continuous values. While discrete variants do exist, I did not explore these. The simplest way to extend this work to discrete datasets is to use a discrete VAE to encode samples into a continuous latent space and perform diffusion there instead.


#+BEGIN_EXPORT html
<div id="images">
<br />
<figure>
<img class="figg" src="/assets/mbo/mbo-scatterplot-figures.png" width="700" alt="" />
</figure>
<figcaption><b>Figure 5.</b> Each dataset is a subfigure, and each column is a validation metric. Each metric is plotted against <i>M_reward</i> (shown here as <i>M_test_reward</i>), and points are colour-coded according to a diffusion-model specific generation parameter called label guidance (which can be safely ignored for this figure). The Pearson correlation ρ is shown above each plot. Since each validation metric in this figure is designed to be minimised (negative values are added to metrics which typically are maximised), we are interested in metrics which give rise to the most negative values of ρ.</figcaption>
<br />
</div>
#+END_EXPORT

Here, some metrics are plotted as their negatives, e.g. $-\mathcal{M}_{\text{DC}}$ and $-\mathcal{M}_{\text{reward}}$. This is because these metrics are quantities which are ideally maximised, so we must take their negative to ensure all metrics are framed as quantities which should ideally be minimised. Since we still view $\mathcal{M}_{\text{test-reward}}$ (Eqn. 1) as something which is to be /maximised/, we really want to find which metrics are most negatively correlated with it.

Since the above plots are a lot of information to process, we can just jump straight to the figure which barplots the Pearson correlation for each of these experiments:

#+BEGIN_EXPORT html
<div id="images">
<br />
<figure>
<img class="figg" src="/assets/mbo/mbo-barplot.png" width="700" alt="" />
</figure>
<br />
</div>
#+END_EXPORT

/(For those inclined: the difference between "c.f.g" and "c.g." simply refer to classifier-free and classifier-based guidance, as these are two ways to formulate conditional DDPMs. I wanted to explore both formulations for the four datasets.)/

The above figure differs a little from the one before it, as we actually have three additional groups of experiments on the right corresponding to "c.g." in parentheses. These correspond to the "classifier guidance" variant of diffusion [cite:@dhariwal2021diffusion]. I won't go into details here, but you can think of this variant as really defining the joint $\pt(x,y) \propto p_{\beta}(y|x)^{w}\pt(x)$ where $p_{\beta}(y|x)$ is actually the /training/ surrogate (the reward model trained only on the training set), whereas the other variant (referred to in the scatterplot as "classifier-free guidance", or "c.f.g.") does not use an external model for the $p(y|x)$ part.

Overall, if we count which validation metric was most negatively correlated with the test reward for each dataset-guidance configuration, agreement is the most performant, followed by Frechet Distance.

** 🪵🔥 reflecting on this work, and future work

In the name of transparency and introspection, I will discuss what I think could have been done better.

The fundamental question we are trying to answer is: what validation metrics are most useful? To do this, we proposed some reasonable metrics a-priori, and ranked which ones were most correlated with the actual ground truth on four datasets. While this work isn’t about setting new benchmark records, the results are very encouraging. That said, it’s worth noting that this correlation is measured on the same data used to select the best metric, so there’s an inherent optimism bias. The ideal thing to do would have been to demonstrate that these metrics perform well on other downstream tasks. Alas, it was a long project, and I needed to get it done.

# I simultaneously motivate maintaining the /spirit/ of offline MBO (i.e. don't abuse the reward oracle, treat it as sacred!), yet also propose breaking this rule because I really do believe there is value in exploiting simulation environments to help us with real world MBO tasks. But it's a difficult one to reconcile.

This paper was tough to write, and honestly, so was this blog post. Maybe it’s because there were just too many ideas bouncing around at once: the importance of using a proper validation set, how to design that set to test extrapolation, thinking about MBO through the lens of generative modeling, and on top of that, proposing diffusion models -- which, at the time, hadn’t really been explored in offline MBO. It would’ve been much simpler to just stick to the message of "validation sets, but for extrapolation". But at the time, that felt almost too obvious, like writing a paper just to say that validation sets are useful, which we already take for granted.

I'm not sure how I would have approached the project if I did it again. When I reflect on past and ongoing industry work in similar problems, I notice a recurring pattern: we lean heavily on proxy metrics for validation. They’re cheap, measurable, and give us a sense of progress. But they’re also brittle and riddled with edge cases. It’s hard not to conclude that, sooner or later, all roads lead back to human feedback. It’s the only metric that consistently captures what we actually care about, even if it's noisy and expensive. So what does this mean then for this type of research? Maybe we ought to focus on the validation proxy directly and incorporate the best of both worlds: reliable, domain-specific priors, but also cheaper sources of human feedback (for instance, pairwise comparisons, rather than directly assigning scores to inputs). Cost is still an issue, but based on current trends it seems like the way forward involves low-shot finetuning on top of very powerful foundation models, as these are most label efficient.

*** online and offline, put together

Lastly, online and offline MBO feel somewhat siloed, when really one leads to another. Ideally, we want to build a /good inductive prior/ in the offline setting and then segue into online to refine the model with /real interactions/. But in practice, offline MBO is only concerned with models which produce high-scoring samples "out of the box" with respect to the ground truth, not whether that same model can be effectively used by an online learning algorithm to sample better inputs more efficiently.

A real-world MBO workflow might appear as the following:
- (1) We start with offline data, e.g. past experiments, human preferences, etc.
- (2) Train a generative model on that data.
- (3) "Use generative model" + search algorithm to propose a small batch of high-scoring candidates and query those candidates with the ground truth oracle to obtain their labels.
- (4) Add the newly-obtained (input, label) pairs to the dataset.
- (5) Retrain the generative model or fine-tune.
- (6) Repeat steps (3)-(6) until budget is exhausted.

For (3), examples of "use generative model" may include:
- Using the generative model as a prior, e.g. if $\pt(x,y)=\pt(x|y)p(y)$, then the search algorithm can initialise its starting point via a sample from $\pt(x|y)$.
- If the generative model is an unconditional $\pt(x)$, the search algorithm evaluates the density of an input $\pt(x)$. This is possible with certain models such as time-continuous diffusion [cite:@song2020score] and normalising flows.

Here is a more pragmatic and concrete sketch of this algorithm. To avoid any bias due to optimism, we use the validation surrogate $\fvalid$ as the ground truth, and save the final evaluation with $f$ until the very end.

- (1) Assume offline dataset $\mathcal{D}$, split into $\dtrain$, $\dvalid$, and $\dtest$. $\fvalid$ is also trained on $\dtrain \cup \dvalid$.
- (2) Train $\pt$ on $\dtrain$. (Here, $\pt$ can refer to any density deemed useful, for instance $\pt(x)$, or $\pt(x,y)$.)
- (4) **Online mode.** For timestep $t = 1, \dots, T$:
  - (4a) Use $\pt$ with search algorithm to sample high-scoring candidates.
  - (4b) Obtain labels of candidates with $\fvalid$, compute mean reward $r_t$, save this value.
  - (4c) Update $\tilde{\mathcal{D}}$ with previously obtained labels and fine-tune $\pt$ with it.
- (5) Compute /discounted/ sum of reward: $G_T = \sum_{t=1}^{T} \gamma^{t-1}r_t$ for discount rate $\gamma$.

I also take an idea from RL which is the /discounted sum of rewards/. This sum is meant to encode the notion that rewards obtained earlier are better than later, as each evaluation progressively increases the /overall cost/ of the algorithm. By finding good hyperparameters for $\pt$ which maximise this sum, we select for models which produce good samples as early as possible, which is when fewer evaluations are performed.

(Hats off to my co-author Alex, who really instilled a sense of MBO needing to be cost-effective. I think this algorithm really hits at the heart of that.)

** open source

Here are some things you may find useful:
- 🛠️ [[https://github.com/christopher-beckham/validation-metrics-offline-mbo][[validation-metrics-offline-mbo]​]]: the original code for my paper. This uses the DDPM style of diffusion model from Ho et al.
- 🛠️ [[https://github.com/christopher-beckham/offline-mbo-edm][[offline-mbo-edm]​]]: this is a bit more minimalistic and has a more up-to-date diffusion model which is EDM. Not only is this more performant, it generalises existing diffusion models which grants a lot of flexibility when it comes to deciding how to sample.

Also worth noting -- Design Bench can also be a pain to setup, so whichever repo you look at I highly recommend you consult the installation readme [[https://github.com/christopher-beckham/offline-mbo-edm/blob/master/INSTALL.org][here]]. As of time of writing, the mainline branch for Design Bench has broken urls for its datasets, so you should switch to my branch:

#+BEGIN_SRC bash
git clone https://github.com/brandontrabucco/design-bench
git checkout chris/fixes-v2
cd design-bench
pip install . -e
#+END_SRC


* References

#+print_bibliography:
