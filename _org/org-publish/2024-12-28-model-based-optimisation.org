#+OPTIONS: toc:nil
#+LATEX_HEADER: \newcommand{\ft}{f_{\theta}}
#+LATEX_HEADER: \newcommand{\ftrain}{f_{\text{train}}}
#+LATEX_HEADER: \newcommand{\fvalid}{f_{\text{valid}}}
#+LATEX_HEADER: \newcommand{\ftest}{f_{\text{test}}}
#+LATEX_HEADER: \newcommand{\fphi}{f_{\phi}}
#+LATEX_HEADER: \newcommand{\ds}{\mathcal{D}}
#+LATEX_HEADER: \newcommand{\pt}{p_{\theta}}
#+LATEX_HEADER: \newcommand{\ptnew}{p_{\theta, \text{valid}}}
#+LATEX_HEADER: \newcommand{\ptrain}{p_{\text{train}}}
#+LATEX_HEADER: \newcommand{\pvalid}{p_{\text{valid}}}
#+LATEX_HEADER: \newcommand{\dtrain}{\mathcal{D}_{\text{train}}}
#+LATEX_HEADER: \newcommand{\dvalid}{\mathcal{D}_{\text{valid}}}
#+LATEX_HEADER: \newcommand{\dtest}{\mathcal{D}_{\text{test}}}
#+LATEX_HEADER: \newcommand{\drest}{\mathcal{D}_{\text{rest}}}
#+LATEX_HEADER: \newcommand{\argmax}{\text{argmax}}
#+LATEX_HEADER: \usepackage{tcolorbox}
#+bibliography: mbo.bib
#+cite_export: csl ieee.csl

#+BEGIN_EXPORT html
---
title: bridging offline and online model-based optimisation
description: TODO
layout: default_latex
---

<h1>bridging offline and online model-based optimisation</h1>

<div hidden>
<!-- This should be consistent with LATEX_HEADER -->
$$\newcommand{\argmax}{\text{argmax}}$$
$$\newcommand{\ft}{f_{\theta}}$$
$$\newcommand{\ftrain}{f_{\text{train}}}$$
$$\newcommand{\fvalid}{f_{\text{valid}}}$$
$$\newcommand{\ftest}{f_{\text{test}}}$$
$$\newcommand{\fphi}{f_{\phi}}$$
$$\newcommand{\ftt}{f_{\theta}}$$
$$\newcommand{\ds}{\mathcal{D}}$$
$$\newcommand{\pt}{p_{\theta}}$$
$$\newcommand{\ptnew}{p_{\theta, \text{valid}}}$$
$$\newcommand{\ptrain}{p_\text{train}}$$
$$\newcommand{\pvalid}{p_\text{valid}}$$
$$\newcommand{\dtrain}{\mathcal{D}_{\text{train}}}$$
$$\newcommand{\dvalid}{\mathcal{D}_{\text{valid}}}$$
$$\newcommand{\dtest}{\mathcal{D}_{\text{test}}}$$
$$\newcommand{\drest}{\mathcal{D}_{\text{rest}}}$$
</div>

#+END_EXPORT

#+BEGIN_EXPORT html
<div id="images">
<br />
<figure>
<img class="figg" src="/assets/mbo/mbo-header.png" width="600" alt="" />
</figure>
<figcaption><b><i>(attribution: cite paper)</i></b></figcaption>
<br />
</div>
#+END_EXPORT

# Some bullshit to be aware of:
# - org-cite-insert doesn't like enter, you have to do C-M-j
#   - See https://www.reddit.com/r/orgmode/comments/q58f4f/how_to_actually_insert_a_citation_with_orgcite/

#+TOC: headlines 3

# In this blog post, I give a brief introduction to model-based optimisation, explain a fundamental research question I tried to pursue last year in the context of /offline/ model-based optimsiation (one half of the problem), and then reflect on that work and how it relates to /online/ (the other half of the problem).

In this post, I introduce offline model-based optimization and share highlights from research I published at the tail end of my PhD. The focus is on issues of validation and extrapolation -- two topics that remain frustratingly underexplored in the field. Along the way, I offer an honest critique of my own work, reflect on broader challenges in offline MBO, and point to a few promising directions forward.

While I‚Äôm no longer working full-time in academia, this topic still matters a lot to me. If any of it resonates with you -- whether you‚Äôre a researcher, engineer, or just curious, I‚Äôd love to chat (albeit in a bandwidth-conscious capacity).

* intro 
:PROPERTIES:
:CUSTOM_ID: sec:intro
:END:


# context: MBO, we want to design inputs, ones which maximise some desiderata which is encoded by a real world reward function.
In model-based optimization (MBO), the goal is to design inputs‚Äîoften tangible, real-world objects‚Äîthat maximize a reward function reflecting some desired property. Crucially, this reward function exists in the real world, meaning that evaluating it requires physically testing the input and observing its outcome. For example, in protein design, the input might be a molecular specification, and the reward could be the protein‚Äôs ability to bind to a receptor. In this setting, the true objective $f(x) \in \mathbb{R}^{+}$ measures binding affinity, and querying it involves running expensive lab experiments.


# online: use the ground truth to guide the search, active labelling
# however, this is expensive
In theory, we could optimise the true reward function $f$ directly by combining it with a search algorithm, learning from past evaluations to guide the search for better inputs. This involves learning a "surrogate" model $\ft$, which is exploited by the search algorithm and also refined iteratively based on feedback provided by the ground truth reward function. This is called /online model-based optimisation/ ("online MBO"). In practice however, querying $f$ is often prohibitively expensive and slow, and we need many samples if we want the model (the surrogate) to learn enough about the underlying problem in order to guide the search towards promising candidates. 

But if we already have an existing dataset of input-reward pairs ($y = f(x)$ is the "reward"), we can use it to /pre-train/ the surrogate model. The idea is that a well-trained surrogate may then require far fewer queries during the online phase to guide the search effectively. This pre-training stage is known as /offline model-based optimisation/ ("offline MBO").

# conclusion: proxy is difficult, mbo is difficult
Since $\ft$ is an approximation however, it is particularly brittle. It can behave unpredictably on regions of input space which are not represented in the training data ("out-of-distribution"), and this is especially problematic since we assume the best input candidates are not already yet in the training set. It is also vulernable to adversarial examples, where by naively optimising for the highest-scoring candidate with respect to the surrogate often produces a candidate which is no longer plausible according to real world constraints. This makes it an interesting yet challenging area of research.

# MBO can be categorised into two varieties, online and offline. In online, we assume that $f$ /can/ be queried during training. One such instance is Bayesian optimisation applied to this setting: we have a GP regression model $\ft$ and the learning algorithm alternates between proposing candidates $x$ (via some search algorithm) and subsequently invoking the ground truth $y = f(x)$. From this, we can treat $(x,y)$ as a newly acquired data point to incrementally update $\ft$ and the process continues.

# Assuming $\ft$ is "expressive" enough and it is economically viable to obtain "enough" samples from $\ft$ (which isn't practical, but more on this later), then surely we can learn a good model.

** formalising offline mbo
:PROPERTIES:
:CUSTOM_ID: sec:intro_whatis
:END:

# context: this is the math describing offline mbo, also we seg into bayes rule
In the offline setting, we assume access to a labelled dataset of input-output pairs, denoted $\mathcal{D} = \{(x_i,y_i)\}_{i=1}^{N}$, where $y_i \in \mathbb{R}^{+}$. The goal is to learn a model of the reward -- typically a surrogate function $\ft$ -- without querying the true function during training or model selection. While much of the offline literature focuses on a surrogate $\ft$, it's arguably more useful to think in terms of a generative model that defines a joint distribution $\pt(x,y)$. Via Bayes' rule, it can factorise in one of two ways.

# content: first factorisation
The first is $\pt(y|x)p(x)$, and we can think of $\pt(y|x)$ as the probabilistic form of the surrogate, i.e. $\ft(x)$ parameterises the mean of the distribution. We can interpret this as: first we sample $x$ from some prior (e.g. the training distribution), then we predict its reward. While this is /technically/ a generative model, it is not particularly useful for sampling high-scoring candidates as the only sampling we do is from the training data (and not a model we have learned).

# content: first factorisation, doesn't make much sense
In practice, a common baseline takes a hybrid approach which doesn't quite correspond cleanly to this. This involves sampling $x$ from the training data, which is then iteratively updated by ascending the gradient of $\ft(x)$ (typically the mean of $\pt(y|x)$). While this produces inputs with higher predicted reward, it abandons the semantics of the above factorisation and tends to produce poor inputs when scored against the actual ground truth reward. (While online MBO also does a sort of hill climbing on the surrogate, the difference is that the resulting input is validated against the ground truth reward function, and this data is used to update the model.)

# content: second factorisation, also it makes more sense
# also conclusion.
The second factorisation is $\pt(x|y)p(y)$, which we can think of as saying: first choose the desired reward $y$, then find an input which has that reward. Since $\pt(x|y)$ is a /conditional generative model/, not only can we target high reward regions, but we can also avoid generating implausible inputs since it is a mechanism built into the model. (While generative models are not by any means invulernable to generating such inputs, the key idea is that plausibility to built into the model by design.)

For the remainder of this work, we will define our joint generative model $\pt(x,y)$ as the second factorisation:

\begin{align}
\pt(x,y) = \pt(x|y)\ptrain(y),
\end{align}

where $\ptrain(y)$ is the empirical distribution over the rewards in the training set, and $\pt$ is also learned from this.

# This framing aligns naturally with /generative models/, which are designed to model the distribution of the data directly. Furthermore, since this is a conditional generative model, we get to have a model which can target both high-reward regions and also avoid generating unrealistic or adversarial inputs. 
# conclusion: 2nd factorisation makes more sense, and generative models fit the task.
# In the offline MBO setting, this is especially appealing. Since it is too expensive to interact with the ground truth reward function during training, we want a model which can both target high-reward regions and avoids generating unrealistic or adversarial inputs. Conditional generative models $\pt(x|y)$ offer a principled and practical way to achieve this. While generative models are not by any means invulernable to generating adversarial or implausible inputs, the key point is that plausibility is built into the model by design.

** ‚ÄºÔ∏è the key idea is extrapolation
:PROPERTIES:
:CUSTOM_ID: sec:intro_extrapolate
:END:

# context: we don't just want to generate, we want to extrapolate, but how do we do this
The key idea which seperates MBO from regular generative modelling is that we don't just want to generate any kind of sample from the model. We would like to generate samples whose /real/ reward $y$ is as large as possible, as these have the most real world utility. The difficulty lies in the fact that these (extremely) high scoring samples do not exist in the training set, otherwise MBO would not be needed to begin with. Furthermore, it means models have to /extrapolate/ -- the model cannot see high scoring samples in the training set in order to produce similar things. Rather, it has to learn what constitutes low and medium-scoring samples, and infer what a high-scoring sample may look like.

# content: explain that we need to change the prior
This also implies that the behaviour of the generative model needs to somehow be 'tweaked' at generation time. For instance, we have defined generative model $\pt(x,y)$ to be the following:

\begin{align}
\pt(x,y) = \pt(x|y)\ptrain(y),
\end{align}

where $\ptrain$ is the empirical distribution of $y$'s observed in training. If we simply sample according to this strategy, we will only sample conditioned on the kinds of reward seen in the training set. To rectify this, we /could/ switch out the prior for another distribution $\widehat{p}(y)$, one which reflects a larger distribution of rewards. For instance, if $\ptrain(y)$ reflects a range of values from $[0,100)$, perhaps the new prior reflects those from $[100,200]$. From this, we can define the "extrapolated" model:

\begin{align}
\widehat{\pt}(x,y) = \pt(x|y)\widehat{p}(y).
\end{align}

(I am using '$\widehat{\pt}$' to symbolise an elevation in the reward we want to condition on.) Ideally we would like to find an "extrapolated" model $\widehat{\pt}(x,y)$ such that it maxmises:

\begin{align}
m_{\text{test}}(p; f)|_{p=\widehat{\pt}} = \mathbb{E}_{x \sim \widehat{\pt}(x,y)} f(x). \tag{1}
\end{align}

In other words, we want to find a $\pt(x|y)$ and $\widehat{p}(y)$ such that samples produced by the former have as large of a reward as possible, according to the ground truth. Since this equation involves $f$ which is too expensive to compute during training or model selection, it is only intended to be executed at the very end of the machine learning pipeline. This means that evaluating Eqn. (1) during those stages is not feasible. 

# content: would this work? it depends on a lot of things.
# Whether or not this model will work on this new prior is not straightforward. Apart from the choice of prior itself, it also depends on the training data, the model's inductive biases, how much it overfits (or underfits), as well as how samples are actually produced from the model. In the latter case, the actual class of generative model already defines a sampling strategy, whether it generates inputs autoregressively, via latent decoding (e.g. GANs), or through iterative denoising (diffusion models). While this sampling process  could, in principle, be folded into a more elaborate search procedure, we keep things simple here and simply abstract it under the notation "$x \sim \pt(x|y)$".


# and we compute a Monte Carlo estimate of Eqn. (1) but by passing in the new generative model and $f$. (The Monte Carlo samples from $\pt$ can be thought of as the "test set".)

To rectify this, we could simply replace $f$ with the surrogate model $\ft$. However, $\ft$ has also only been trained on the same empirical distribution of rewards, and we cannot expect it to score inputs conditioned on e.g. $[100,200]$ reliably, as this is clearly out-of-distribution.

One approach is to split the dataset into low-to-moderate scoring examples and high-scoring examples. For instance, if our /original/ dataset only represented samples with reward in $[0,100]$, then we could for instance split it into $[0,50]$ (low-to-moderate) and $[50,100]$ for high scoring (see Fig. 1). The low-to-moderate split is used to train the generative model, while the latter forms a validation set. The combined sets (which is just the full dataset) is used to train a /validation surrogate/, $\fvalid$.

To evaluate the generative model's ability to extrapolate, we simply run its "extrapolated" variant (sample from $\widehat{p}(y)$), and the corresponding samples can be effectively validated by the validation surrogate precisely because it already saw those range of rewards during training. Therefore, this setup allows us to measure not just generalization, but generalization specifically in the context of /reward extrapolation./


#+BEGIN_EXPORT html
<div id="images">
<br />
<figure>
<img class="figg" src="/assets/mbo/hand-sketched_extrapolation.png" width="500" alt="" />
</figure>
<figcaption><b><i>Figure 1: "Actual data distribution" signifies the real data, where the assumed max reward is 200. Since we assume our offline dataset only contained reward values between 0 and 100, if we want to measure extrapolation then we need to split this into two subsets. In this illustration, 0-50 is chosen for train and 50-100 for validation.</i></b></figcaption>
<br />
</div>
#+END_EXPORT


# for fundamental ML papers it's impractical to actually execute a real world reward function. Since we typically use prescribed benchmark datasets, the best we can do is train a "ground truth surrogate" $\ftest \approx f$ or use a simulator. Since either formulations are relatively /cheap/ to compute (even multi-billion parameter DNNs are cheaper to eval than a human), researchers can just hammer away at performing model selection on the test oracle. However, this goes against the spirit of offline MBO.

# Design-Bench, the MBO benchmarking framework from which our work is based on, also does not officially prescribe a validation set for any of its datasets, only a training set. In theory, the user /could/ treat Design Bench's "training set" as really "train and validation set" and then re-assign the percentage of it to be the actual training set, but this can either result in not measuring extrapolation or complicate what the resulting splits of the data represent.

# Since $f$ is expensive to evaluate however, it only makes sense to invoke this at the end of model training and selection, i.e. it is the /test metric/ and we only use it to obtain a final unbiased estimate of model performance. Since it would be too expensive and impractical to use in the context of model selection, we would need to use an approximation of $f$ in its place.[fn:2]

** ‚ÄºÔ∏è why evaluation is difficult (and misunderstood)
:PROPERTIES:
:CUSTOM_ID: sec:intro_evaldifficult
:END:

# context: shift in generative modelling -> need to rethink eval
With the rapid progress in generative modeling over the past few years, our approach to evaluation has evolved. In earlier eras of machine learning, it was common to assess models based on likelihood over a test or validation set -- a natural outcome of maximum likelihood estimation, where the goal is to find parameters $\theta$ that maximise the probability of the observed data.

# content (details on likelihood vs sample based eval, how surrogates fit in).
Because of the extremely rapid advances in generative modelling in the past few years, the way we have performed evaluation has changed. In the olden days of machine learning, it was more common to evaluate machine learning models by way of likelihood on a test or validation set. This is a natural consequence of maximum likelihood estimation, which states that we wish to find a model which best explains the data, i.e. find parameters $\theta$ such that the parameterised model assigns the highest likelihood. However, likelihood is only concerned with how plausible /pre-collected samples/ are, rather than whether samples generated from the model itself satisfy a useful notion of preference. Such preferences can be encoded with a reward function $f$, but this is typically expensive to compute as it reflects a real world process (i.e. $y = f(x)$ is like asking a human rater to evaluate $x$).

# üìö Here's an example: We wish to learn a conditional generative model $p(x|y_1, \dots, y_m)$ to produce floorplans. Since it is a conditional model, we can condition on attributes such as the number of desired rooms, and architectural style. The simplest form of evaluation is to just evaluate the log likelihood on a held-out test set of /real-world/ floorplans, but we want to actually use the model to generate actual floorplans intended for construction and use in the real world. If we generate samples, we inherently no longer have their ground truth attributes, those plans have to be validated via human feedback. This is basically us "executing" $f(x)$, it is a human feedback mechanism.

# conclusion: validation is hard and underexplored.
As mentioned in Sec. [[#sec:intro_extrapolate]], a principled strategy is to approximate $f$ with $\fvalid$ and continue forward. Even if $\fvalid$ is an approximation, it actually serves as a useful anchor for the generative model. This is because even though it is only trained on low-to-moderate scoring inputs, we can measure its ability to generate high-scoring inputs against the validation surrogate which has technically seen high scoring inputs during training. Compared to other MBO literature, I make a very explicit distinction between /validation/ and /testing/ which does not seem to be well-respected in offline MBO literature, and I partly suspect it's because there is a conflation between /"real world" MBO/ and /"academic" MBO/. 

# üìö /(Example 2: You are fine-tuning and evaluating an LLM to summarise research papers. This requires a rigorous evaluation of the model's ability to accurately distill the paper in a paragraph without hallucinations. While there may exist cheap metrics to quantify this, they are heuristic or may easily be blinded by edge cases, and therefore ultimately do not allow you to have full confidence in the LLM. While the use of human raters is costly and laborious, you also understand that it is absolutely necessary to have full confidence in the model.)/


By "academic MBO" I simply mean doing MBO in the context of academic research, i.e. publishing papers. In this situation it may not be practically feasible to evaluate the real ground truth, for instance in the case where the benchmark data involves an extremely expensive human evaluation (e.g. protein synthesis). To rectify this, some MBO datasets provide a corresponding simulator, which for all intents and purposes can be treated as a ground truth. Since the simulator is just a function that can be freely executed /in silico/ with negligible monetary cost, researchers can (intentionally or not) violate the spirit of offline MBO by "abusing" the simulator and constantly testing it against whatever model is being trained. This is especially enticing in academia because there is an overwhelming bias towards pushing things that "beat SOTA" or are "novel".

#+BEGIN_EXPORT html
<div id="images">
<br />
<figure>
<img class="figg" src="/assets/mbo/hand-sketched_testset.png" width="500" alt="" />
</figure>
<figcaption><b><i>Figure 2: In "academic MBO", what is meant to be treated as an expensive-to-evaluate ground truth reward function is not treated as such, since it doesn't truly represent a real world process. It can either take the form of a simulation environment (which is significantly cheaper to compute than a real world process), or a neural approximation trained on held-out data (e.g. a test set), which is also cheap to compute. Conversely, in "real world" MBO, the ground truth is truly too expensive to compute for training  or model selection, so a validation set is needed.</i></b></figcaption>
<br />
</div>
#+END_EXPORT


Conversely, in "real world" MBO there is already a safeguard against abusing the ground truth and that is money. You can't just blast away thousands of dollars for every single hyperparameter configuration you wish to explore.  We will elaborate on this in the next section.

** the train/val/test recipe, moving forward
:PROPERTIES:
:CUSTOM_ID: sec:intro_summary
:END:

As discussed in Sec. [[#sec:intro_extrapolate]], we need to measure not just generalisation, but extrapolation. If our validation set follows the proposed setup in Fig. (1), then we can just approximate Eqn. (1) via the following:

\begin{align}
m_{\text{test}}(\widehat{\pt}; f)_{|f = \fvalid} = \mathbb{E}_{x \sim \widehat{\pt}(x,y)} \fvalid(x), \tag{2}
\end{align}

which constitutes our first validation metric. By "validation metric" I simply mean some function which measures the "goodness" of the generative model $\pt$ using either the validation set, surrogate, or both. We will discuss some other ones later.

<<ref-fvalid-concern>> Note that while Eqn. (2) is a principled and reasonable approach, one may wonder whether it is always appropriate. Even if $\fvalid$ satisfies our desire in measuring extrapolation and avoiding the ground truth, it ultimately shares the same limitations as any other neural discriminative model, in the sense that it is often vulernable to adversarial examples or being overconfident. Therefore, validation metrics go beyond just simply directly approximating Eqn. (1), and may involve measuring other aspects of the generative model or data.

# How validation sets are handled in existing offline MBO literature is unfortunately not clear. As an example, some existing works   [cite:@fannjiang2020autofocused;@brookes2019conditioning;@mins] make use of a training split as well as an approximate "test surrogate" model trained on the full dataset, but as mentioned, if this is used in model selection then it violates the spirit of offline MBO. As such, I suggest the following strategy, even though it appears to be "non-standard". Given our dataset $\mathcal{D}$:

Based on everything we discussed so far -- the need for extrapolation (Sec [[#sec:intro_extrapolate]]), as well as the confusion between academic and real-world MBO (Sec [[#sec:intro_evaldifficult]]) -- a reasonable train-then-validate-then-test recipe is the following:

- *Inputs*: Split data into: $\dtrain$, $\dvalid$, and $\dtest$.
- *Training*: The generative model is /only trained/ on $\dtrain$.
- *Validation*: $\dvalid$ is used for model selection / validation given some validation metric (Eqn. (2) is one /example/ of this.)
  - Validation metrics may also rely on a "validation surrogate", which is $\fvalid$. 
  - If necessary, one may also train a "validation surrogate" $\fvalid$ on $\dtrain \cup \dvalid$, and use this as part of model selection as well.
- *Final evaluation*:
  - If we're doing "academic MBO", evaluate on whatever is meant to be the expensive-to-evaluate (but not really) ground truth reward function (Fig. (2)).
  - If we're doing "real world MBO", then you will need to confront the real ground truth $f$ (this costs money!).

In summary:

- (1) In offline model-based optimisation we wish to learn a reward-conditioned generative model from a dataset of input-reward pairs. It is assumed that the rewards come from a "ground truth" reward function which is too expensive to compute during training or validation.
- (2) Evaluating samples from a generative model is a difficult task, and even more so in MBO since we also would like the model to extrapolate and generate conditioned on rewards it has not seen during training.
- (3) Evaluation is difficult, often neglecting a validation set. This may be related to the confusion between "real world" and "academic" MBO. In the latter case, ground truth reward function simply gets replaced with an approximate "surrogate ground truth" or simulator which is cheap to compute in-silico, so it is tempting to just use that instead. The principled solution is to use a surrogate reward model for validation which is clearly distinct from the surrogate ground truth.
- (4) How can we find cheap-to-compute validation metrics to help us search for good models?



** related work

# $\mathcal{D} = \dtrain \cup \drest$ ($\drest$ contains the higher scoring examples), then the following models should be trained as follows:
# - A validation and set is randomly subsampled from $\drest$ without replacement;
# - The validation surrogate $\fvalid$ is trained on $\dtrain \cup \dvalid$, and the test surrogate is trained on $\dtrain \cup \dvalid \cup \dtest$, which is just the entire dataset $\mathcal{D}$.
# - Train generative model $\pt$ on $\dtrain$, validate on either $\dvalid$ or $\fvalid$ (or both), and compute final test reward on $\ftest$.

# üìö /(This also parallels reward model finetuning for LLMs: to have a principled evaluation, one can train three reward models: $\ft(x)$ for the actual RLHF procedure, $\fvalid(x)$ for validation, and $\ftest(x)$ for actual testing at the end./)

# Assuming part of our dataset is set aside as a training split, the simplest solution is to train a "validation surrogate" on the full dataset and then train the generative model on the training set. Then we can use the former to validate the latter. In a "real world" MBO setup we would just defer testing at the end to the actual ground truth. Otherwise, in "academic" MBO we should have both a "validation" and "testing" surrogate, with the latter being trained on more data than the former.


# /üìö (Going back to the LLM example, the validation metric may also be an approximate reward model which emulates human feedback, i.e. $\fvalid \approx f$. But it may also take other forms, for instance measuring some semantic distance (e.g. BLEU score) between samples from the LLM and those in the validation set. We use it not because we think it's the best evaluation metric, but because it's pragmatic.)/

# More formally, suppose we denote an evaluation metric as $\mathcal{M}: \mathbb{P}_{\Theta} \times \mathcal{F} \rightarrow \mathbb{R}$ where $\pt \in \mathbb{P}_{\Theta}$ and $f \in \mathcal{F}$, then

# describes the case where we approximate the test reward in Eqn. (1) with the validation surrogate.

# In my work, I formalised this problem as first defining a-priori some metrics $m_i$, and then ranking them by seeing how well they are correlated with $m_{\text{test}}$. Since it is a function of $f$ however it isn't practical. We can work around this issue with simulation datasets where it is actually cheap to compute.[fn:3] Simulation environments give cheap access to $f$, but we still shouldn't use it for model selection because it no longer becomes faithful to a real-world MBO problem, one in which that kind of function is actually expensive to compute. If we are doing research involving simulation environments however, then it seems reasonable to exploit the fact that $f$ is cheap-to-compute and find good validation proxies to use in place of it, which can be used a-priori in real-world MBO tasks. This is what I will cover in the next section where I explain more of last year's work.

#+BEGIN_COMMENT
\paragraph{Use of validation set} Compared to other works, the use of a validation set varies and sometimes details surrounding how the data is split is opaque. For example, in \cite{mins} there is no mention of a training or validation set; rather, we assume that only $\dtrain$ and $\dtest$ exists, with the generative model being trained on the former and test oracle on the latter (note that if the test oracle is approximate there is no need for a $\dtest$). This also appears to be the case for \cite{fannjiang2020autofocused}. While Design Bench was proposed to standardise evaluation, its API does not prescribe a validation set\footnote{However, in \cite{trabucco2022designbench} (their Appendix F) some examples are given as to what validation metrics could be used.}. While the training set could in principle be subsetted into a smaller training set and a validation set (such as in \cite{qi2022data}), the latter would no longer carry the same semantic meaning as \emph{our notion} of a validation set, which is intentionally designed to \emph{not be} from the same distribution as the training set. Instead, our evaluation framework code accesses the \emph{full} dataset via an internal method call to Design Bench, and we construct our own validation set from it. We illustrate these differences in Figure \ref{fig:mbo_data_splits}.
#+END_COMMENT

# - Model-based optimisation (or more generally, problems where we want to evaluate quality of samples from the model) is difficult because we don't know the labels of the samples we generate, and that requires the ground truth. (Contrast this to more "traditional" ML where we have a test set which is clearly labelled and can evaluate things on it.)
# - In offline MBO we are given a dataset of labelled examples but cannot assume access to the ground truth for training nor model selection.
# - The test metric is the expected reward of the samples wrt to the ground truth, but we cannot also use this for validation because it's too expensive to compute. Even in the "academic" setting it is not faithful to the original problem.
# - We cannot make the validation metric the same as the test metric. We could substitute $f$ with an approximation, or ponder whether there are better alternatives.
# - The work I published explored the latter, work in which we define reasonable validation metrics to start, and then rank these with how correlated they are with the ground truth. In order to $f$ to be cheap to compute however, we resort to simulation datasets of a similar kind to those in RL.

# ** model validation

# (TODO add figure here showing the cylinders)

# Let us assume $f$ is a real-world function. We want to train a generative model $\pt(x,y)$ from which we can draw samples. Just like any other machine learning workflow, model selection is super important and this depends on a validation metric. Since $f$ is too expensive and impractical to use in the context of model selection, we can use some approximation of it (a "validation surrogate") to help us tune parameters. This would be akin to taking either Eqn. (1) or (2) and replacing $f$ with $\fvalid$. Once model selection is completed, we can compute the final estimate of the generative model's performance by invoking the original Eqns. (1) or (2). which are defined with respect to the ground truth $f$.

# To summarise:
# - We train a joint generative model $\pt(x,y)$ on the training set $\dtrain$;
# - We train the "validation surrogate" $\fvalid$ on the union of the training and validation set (which is just the entire dataset $\mathcal{D} = \dtrain \cup \dvalid$);
# - Model selection.

# For instance, if our dataset is split into a training and validation set, the generative model would be trained on the training set, and the validation surrogate trained on both splits. Once model selection is completed, we can then compute

# It would be too expensive and impractical to use it for hyperparameter tuning (model selection) for the generative model $\pt(x,y)$, so we could instead train a surrogate $\fvalid$. The way $\fvalid$ is used for hyperparameter tuning for the generative model is an open research question.

# To avoid using it, we could use a surrogate model in replace of the ground truth which has been trained on the entire dataset $\mathcal{D}$. Let us call it $\ftest$. Then we train our generative model $\pt(x,y)$ on the training set and use $\ftest$ to score it. At the same time, we don't want to use $\ftest$ for model selection since that would give biased estimates of performance as well, so we instead use a surrogate specifically for model validation called $\fvalid$. Assuming the offline dataset was randomly subsampled into $\dtrain$, $\dvalid$, we could do the following:

[fn:1] The validation metric and test metric here cannot be the same, since the latter relies on expensive-to-compute $f$. This issue can also be seen in other domains, for instance in LLMs the validation metric is a cheap to compute proxy like BLEU score, while the test metric involves human feedback.

[fn:2] Technically, the test metric (Eqn. (1)) could just be a function of a "test surrogate" model $\ftest$ (for instance, if the data is cut up into train / valid / test, train $\fvalid$ on {train,valid} and train $\ftest$ on {train,valid,test}, however now we have to accept that there is a degree of uncertainty involved with the test metric as well.

[fn:3] A similar thing happens in reinforcement learning.

# - one fundamental question is which validation metric to use?
# - can be chosen a-priori or we could find some good ones
# - problem is that this mission would require f which is too expensive, or we can use ftest but it's approximate.
# - could side-step the issue by using simulation environments.

# In terms of obtaining an unbiased estimation of generalisation, we use $\ftest$ and it takes no part in model selection. However, $\ftest$ is also an approximate model. Since $\ftest$ is an approximation there is a risk of it over-scoring examples from the generative model and giving misleading performance estimates. If $f$ is a real-world process then this is a compromise we must live with. Otherwise, we could instead turn to simulation environments, one where $f$ actually lives "in silico" (as in, computer code) but it has the property that it is "exact", which is to say that it "more or less" produces the correct answer for any $x \in \mathcal{X}$. In MBO, the different "flavours" of dataset are the following:

The types of datasets
- (1) Simulations of real-world phenomena, for instance reinforcement learning environments. In [cite], some examples involve optimising for robot morphologies which are then used with a pre-specified policy to measure how far it can run.
- (2) Real-world phenomena, e.g. superconductors, but the ground truth comes from the real world and so the best can do is use a test surrogate $\ftest$.
- (3) Synthetic functions (e.g. see X). These functions are commonly used to test optimisation algorithms, however these are well-supported within the input space or a large hypercube and can make it difficult for generative models to learn any structure in the data.

# In the case of (1), we do have accessible and cheap to compute ground truth. Furthernmor

While (2) is most representative of a real world MBO problem, we can exploit the "in silico" datasets of (1) and take advantage of the fact that the ground truth is easily available. This motivated the work I published where I wanted to devise a principled method for finding validation metrics which are highly-correlated with the ground truth. If we could find such metrics, then we could use them in real world MBO pipelines where the ground truth isn't easily available.


# and try to find validation metrics which are well correlated with them. These could potentially be useful in the real world when we're actually confronted with a real world ground truth.

# In the work I published, I proposed that we find validation metrics which are well correlated with the ground truth, albeit under the situation where the generative model needs to extrapolate beyond its traing distribution. This is because in MBO we want to sample candidates which truly have a large reward $f(x)$, possibly larger than any reward seen in the training set. While the generative model should indeed be able to perform well in-distribution, we ultimately would like it to also perform well out-of-distribution.

# (Relate this back to test score equation.)

# I wanted to consider the exact case, but ask a fundamental question pertaining to model selection: what are some good validation metrics we can use? A validation metric here is simply what we use for model selection, some function which takes as input the generatve model, the validation set, and validation surrogate. I argued that a "good" validation metric is actually one which selects for a generative model which extrapolates well beyond the training distribution. This also requires us to modify the train/val/test split.

# In summary:
# - In a "traditional" ML training and evaluation pipeline -- one which consists of a training, validation, and test split -- the test data already contains labelled examples. This can be easily used to compute the likelihood (or some alternative to it) of the data with respect to the model parameters. **In other words, the generative model needs to explain the test data.**
# - In offline MBO, the "test data" is not prescribed, it has to be generated from the generative model trained on the training set. In order to evaluate how good it is, the ground truth $f$ must be used to evaluate the likelihood of the model's samples, but $f$ is prohibitively expensive to compute. **In other words, the ground truth needs to explain the generative model data.**
# - If the dataset is derived from a real world process (so $f$ characterises a real world process) then the best we can do is use an approximation $\ftest$, otherwise we can turn to simulation environments where $f$ is cheap to compute.
# - Information can be exploited from these functions and used to inform the use of validation metric. Since the validation metric is used for model selection, this can select for models which perform well under that ground truth. These metrics can then be used in real world scenarios.
# - Furthermore, I argue that the typical train/valid/test split breaks iid.

# it would be impractical to invoke in the context of a train/val loop.
# 

# - If $f$ is real world then once we have trained a model, either execute its real world process (expensive, but exact) or use $\ftest$ for something cheaper (cheap, but approximate).
# - If we are dealing with simulation environments then $f$ is cheap to compute, however it would still not be representative of real world MBO to use it for model selection. Instead, we should identify cheap-to-compute validation metrics which we could substitute in place of it.

# that we use a train/valid/test setup in order to be principled about generalisation performance. In addition, I proposed that we find validation metrics (functions of $\pt$, $dvalid$, and $fvalid$) which correlate well with $f$.

# Furthermore, assuming the existence of a ground truth model is the opposite of what typically happens in a typical machine learning pipeline. Usually, we already have a labelled test set on hand and we want to compute some metric which captures some ability of the model to explain the data. We could express this probabilistically as:

# \begin{align}
# \text{conditional log likelihood} = \frac{1}{n} \sum_{i=1}^{n} \log p_{\theta}(Y_i|X_i), \tag{2} 
# \end{align}

# which is computing how likely the ground truth reward $Y_i$ is given $X_i$, with respect to the generative model. Notice how this is tractable because the test set is finite and $\pt$ we already have on hand. Eqn. (1) in turn would be more akin to Eqn. (2) but with $\pt$ replaced with $p$:

# \begin{align}
# \frac{1}{n} \sum_{i=1}^{n} \log p(Y_i|X_i). \tag{3}
# \end{align}

# (which is usually based on the principle of maximum likelihood). For instance, traditionally we already have a labelled test set a-priori, and we want to measure the likelihood of the test data assuming the model parameters. One such example:
# \begin{align}
# \text{test log likelihood}(X|\theta) = \frac{1}{n} \sum_{i=1}^{n} \log p_{\theta}(X_i, Y_i), \tag{2} 
# \end{align}

# for some arbitrary generative model $\pt(x,y)$ of interest. Note that this equation /does not/ assume the probabilistic ground truth $p(x,y)$ or any other ground truth like $f$.

# Conversely, in MBO the idea of a "test set" is less straightforward. Of course, the generative model /should/ generalise and assign high likelihood to a test set if we have one, but it is arguably an intermediate step for an end goal, which is actually using the generative model to /sample/ new inputs. However, if we sample new inputs we don't have their true labels, and it isn't feasible to take Eqn. (2) and swap out $\pt$ for $p$ (if we want likelihood) or $f$ (the actual reward).

# which means we have to use something else in place of it. Hyperparameter tuning is /just as important/ as training and so we shouldn't dismiss it. We could set a-priori some validation metric -- one which is informed by both the class of generative model as well as the problem domain -- but in the paper published last year I thought more deeply about what an "ideal" validation metric might look like. We would like to have a validation metric which is "well correlated" with the test score, but in order to quantiatively determine how true that is then you would need the ground truth function, and if you have it then it negates the purpose behind a validation metric. (It's like trying to set aside a budget to buy a really nice car, but then someone just outright gifts you the car.) My paper looked at how we could break this circular logic, which is that we try to make use of datasets with simulators.

* last year's work
:PROPERTIES:
:CUSTOM_ID: sec:last_year
:END:

The work I published last year addresses the last three bullet points of Sec. [[#sec:intro_summary]]. Firstly, we want our models to /extrapolate/. Secondly, evaluation is /ill-defined/ and seems to conflate real world and academic MBO. I proposed a way to address both of these issues at once, and that involves two things. To address the first, we define a validation set and/or validation surrogate (/super obvious/, I know), and secondly, we break the i.i.d. assumption which typically governs training and validation sets. Instead, the validation set should be defined such that it contains larger rewards than in the training set. Since the model only sees examples from the training set, measuring its performance on a validation set of higher-reward samples is akin to measuring how well it extrapolates.

# Since validation metrics are also functions of the validation split, they would also be measuring the ability of the model to extrapolate. How we find good validation metrics will be explained later.

In Sec. X we described a pretty typical train-valid-test split of the data, one which should be universally applied (though in practice not always) to any machine learning workflow. To make this more concrete, let us suppose that our dataset is $\mathcal{D}$, we could define some threshold $\gamma$ such that the training set $\dtrain$ is all samples whose $y$'s are less than $\gamma$ and validation set are all samples whose $y$ are greater than or equal to $\gamma$. In fact, this is how training sets are defined in Design Bench [cite:@trabucco2022designbench], which is the benchmarking library I used to run experiments for this work. However, Design Bench's API only exposes a training split, which makes it difficult to introduce a validation set into experiments while also being fair with how other papers train and evaluate models. Two compromises are:

- (1) Simply hold out some small part of the training set as the validation set. This respects its API, but effectively reduces the size of the training set and may make it difficult for trained models to be competitive with the literature. (In Fig. 1 left, $\dtrain$ is shown here, so imagine cutting out some portion of this as the validation set.)
- (2) Extract the full dataset internally, disregard the training examples (i.e. all examples whose $y$ is $\leq \gamma$), and set a percentage of this assigned for validation (shown in Fig. 1 right as $\dvalid$, in green). This violates the API in a sense, but it means the training set stays consistent with other works which implement their experiments with Design Bench.

#+BEGIN_EXPORT html
<div id="images">
<br />
<figure>
<img class="figg" src="/assets/mbo/split1.png" width="300" alt="" /> &nbsp; &nbsp; <img class="figg" src="/assets/mbo/split2.png" width="300" alt="" /> 
</figure>
<figcaption><b><i>Figure 1: (left): Design-Bench's prescribed training setup only permits access to the training set; (right) in order for us to perform model selection (and measure extrapolation), I proposed setting aside a validation set which is not from the same distribution as train.</i></b></figcaption>
<br />
</div>
#+END_EXPORT

In my work I decided with (2). We now finally have a validation set! The only thing that is left is to define the validation metrics. From this, a reasonable train/val/test pipeline would be:

- **Training**: train $\pt(x,y) = \pt(x|y)\ptrain(y)$ on $\dtrain$, where $\ptrain(y)$ is the empirical distribution over $y$'s for the training set.
- <<ref-bullet-validation>> **Validation**: Switch out $\ptrain(y)$ for $\pvalid(y)$, which defines a new generative model $\ptnew(x,y)$. Use this in conjunction with a validation metric. We will define a few of these later, but we may also assume that any of these metrics may /also/ be a function of a validation surrogate.
- **Test**: once the best $\ptnew$ is determined according to the validation metric, finally score the model on the real ground truth by invoking Eqn. (1). For "real world MBO", this is the ground truth $f(x)$, for "academic MBO" this is the "test surrogate", $\ftest(x)$.

** ranking validation metrics
  :PROPERTIES:
  :CUSTOM_ID: sec:last_year_valid_metrics
  :END:

Now, all that is left is a validation metric. All I mean by this is something we can use to measure how well the generative model performs. This metric is a function of the generative model $\pt$, and at least one of either the validation set $\dvalid$ and the validation surrogate $\fvalid$. We already saw one of these metrics, which is simply Eqn. (1) but with $\fvalid$ substituted for $f$:

\begin{align}
m_{\text{test}}(\pt; f)_{|f = \fvalid} = \mathbb{E}_{x \sim \ptnew(x)} f(x) \tag{3}
\end{align}

and this is a function of just the generative model, the validation surrogate, and also /part/ of the validation set (the empirical distribution over $y$). This metric doesn't particularly care about how "calibrated" the model is. for instance, if we condition on $y = 50$ and get an example whose reward according to $\fvalid$ is $1000$, the model doesn't get penalised. What is being selected for is simply a model which produces as large of a reward as possible, on average. Otherwise, if this is concerning, one validation metric which is particularly intuitive here is the "agreement" [cite:@mins], which measures the extent to which the validation surrogate agrees with the supposed label of the input generated by the model:

$$m_{\text{agreement}}(\pt; f)_{|f=\fvalid} = \mathbb{E}_{p_{\text{valid}}(y), \tilde{x} \sim \pt(x|y)} (y - f(\tilde{x}))^2. \tag{4}$$ 

Here we are sampling $y$'s from the validation distribution and those values have not been seen by the generative model during training. When we generate inputs conditioned on such values, we want to measure to what extent the validation surrogate agrees that it is indeed the correct label.

# https://chatgpt.com/c/67a77ee2-5fbc-8008-b434-62a547cfed98

Let's now briefly introduce the other validation metrics. To make things more flexible, I also extend the definition of a metric to also be able to condition on a /dataset/. This opens up the possibility of using evaluation metrics which compare distributions of data, and by extension we can also leverage these to measure extrapolation if we allow the validation set to also be conditioned on.

Other validation metrics I defined were:

- $\mathcal{M}_{\text{FD}}$: Frechet Distance ("FD") ([cite:@dowson1982frechet; @ttur]) between the distribution of generative model samples and the validation set. Note that this is /not/ the same as Frechet /Inception/ Distance ("FID"), which uses the ImageNet-pretrained Inception network as a feature extractor. Here, our feature extractor is the feature bottleneck of the validation surrogate.
- $\mathcal{M}_{\text{C-DSM}}$: The noise prediction loss [cite:@ho2020denoising] but evaluated on the validation set. This loss is derived directly from the evidence lower bound of DDPM, which is based on forward KL divergence.
- $\mathcal{M}_{\text{PR}}$: The precision and recall metric proposed in [cite:@kynkaanniemi2019improved].

The last question I wanted to ask in this work was: what validation metrics actually work best, and how do we measure that? I'm not searching for a silver bullet metric -- I'm well aware of the ‚Äúno free lunch‚Äù theorem -- but I am interested in whether some metrics are more consistently reliable than others across tasks.

Of course, evaluating validation metrics requires access to the ground truth reward. But here‚Äôs the paradox: if the ground truth is available and cheap to compute, then why bother with validation metrics at all? In real-world MBO, though, querying the true objective is often expensive, so we rely on validation metrics as proxies.

That‚Äôs why simulation environments matter: they give us access to an exact reward function, letting us test how well different validation metrics correlate with the actual ground truth. The idea is to use this setup to run a large-scale comparison of metrics across many simulated datasets, so we can better understand which metrics are most trustworthy when we don‚Äôt have access to the ground truth. Ideally, this gives us actionable guidance for real-world MBO deployments. In Design Bench parlence [cite:@trabucco2022designbench], simulation environments give rise to "exact" oracles, these are simply reward functions which are based directly on the simulation environment, as opposed to neural approximations.

So how do we use the ground-truth function $f$ to evaluate validation metrics? In this work, I focused on DDPMs [cite:@ho2020denoising], due to their flexibility and strong performance in generative modeling. Using a fixed DDPM architecture, I varied several hyperparameters‚Äîsuch as network width, reward dropout probability, and reward guidance strength. Each unique hyperparameter setting defines a configuration, and for each configuration, I saved the final model checkpoint using early stopping. From each saved checkpoint, I sampled according to the procedure described in [[ref-bullet-validation][here]]. I then evaluated the generated samples using the ground truth reward $f$ (Eqn. 1) and compared those scores to the values assigned by the validation metric under consideration. To quantify this relationship, I computed the Pearson correlation between the validation metric and the ground-truth reward across all configurations. 

These results are illustrated below for Design-Bench‚Äôs continuously-valued datasets. In particular, Ant, Kitty, and Hopper are simulated environments with exact oracles, making them ideal for this type of analysis. For completeness, I also include Superconductor, which uses a non-exact oracle but still provides a useful point of comparison.

#+BEGIN_EXPORT html
<div id="images">
<br />
<figure>
<img class="figg" src="/assets/mbo/mbo-scatterplot-figures.png" width="700" alt="" />
</figure
<br />
</div>
#+END_EXPORT

Here, some metrics are plotted as their negatives, e.g. $-\mathcal{M}_{\text{DC}}$ and $-\mathcal{M}_{\text{reward}}$. This is because these metrics are quantities which are ideally maximised, so we must take their negative to ensure all metrics are framed as quantities which should ideally be minimised. Since we still view $\mathcal{M}_{\text{test-reward}}$ (Eqn. 1) as something which is to be /maximised/, we really want to find which metrics are most negatively correlated with it.

Since the above plots are a lot of information to process, we can just jump straight to the figure which barplots the Pearson correlation for each of these experiments:

#+BEGIN_EXPORT html
<div id="images">
<br />
<figure>
<img class="figg" src="/assets/mbo/mbo-barplot.png" width="700" alt="" />
</figure>
<br />
</div>
#+END_EXPORT

/(For those inclined: the difference between "c.f.g" and "c.g." simply refer to classifier-free and classifier-based guidance, as these are two ways to formulate conditional DDPMs. I wanted to explore both formulations for the four datasets.)/

The above figure differs a little from the one before it, as we actually have three additional groups of experiments on the right corresponding to "c.g." in parentheses. These correspond to the "classifier guidance" variant of diffusion [cite:@dhariwal2021diffusion]. I won't go into details here, but you can think of this variant as really defining the joint $\pt(x,y) \propto p_{\beta}(y|x)^{w}\pt(x)$ where $p_{\beta}(y|x)$ is actually the /training/ surrogate (the reward model trained only on the training set), whereas the other variant (referred to in the scatterplot as "classifier-free guidance", or "c.f.g.") does not use an external model for the $p(y|x)$ part.

Overall, if we count which validation metric was most negatively correlated with the test reward for each dataset-guidance configuration, agreement is the most performant, followed by Frechet Distance.

** ü™µüî• time for reflection 

In the name of transparency and introspection, I will discuss what I think could have been done better.

The fundamental question we are trying to answer is: what validation metrics correlate best with the ground truth? To do this, we proposed some reasonable metrics a-priori, and ranked which ones were most correlated with the actual ground truth for datasets in Design Bench. While the focus of this paper isn't to smash benchmarks, there is already an 'optimism bias' in the results since we basically optimise for the validation metric (by finding which one correlates best with the ground truth) and report its performance on the same data. In order to break that optimism, we would need to evaluate how well that metric performs on a downstream task (dataset) which was not used in any of the existing experiments. Given the already long amount of time I spent on the project, I decided to defer it to future work.

I also think there were too many ideas being presented in the paper: the importance of using a validation set, designing the validation set for extrapolation, using the language of generative modelling, but also using diffusion models, which, at the time, were seemingly not yet explored in offline MBO. I really should have just focused on two of these, at most.

# - fundamental question trying to answer: what validation metrics correlate the best with the ground truth?
# - to do this, exploit simulation datasets, find good metrics, however there is never the downstream task of actually trying these out, and so the results are in a sense biased because if we say that "agreement" is the best metric for these datasets, then it's because we used the ground truth to verify them. In a sense, I am breaking my own rule about being rigorous with train/val/test, but I do acknowledge in the work that validating on a downstream task is good.
# -----

# - online and offline feel siloed
# - offline says you're stuck with the data, can't query gt
# - offline implies a "one shot and pray" scenario, because we never get to test the online setting.
Lastly, online and offline MBO feel artificially siloed, when really one leads to another. Ideally, we want to build a good inductive prior in the offline setting and then segue into online to refine the model with real interactions. But in practice, offline MBO is only concerned with models which produce high-scoring samples "out of the box" with respect to the ground truth, not whether that same model can be effectively used by an online learning algorithm to sample better inputs more efficiently.

A real-world MBO workflow might appear as the following:
- (1) We start with offline data, e.g. past experiments, human preferences, etc.
- (2) Train a generative model on the data.
- (3) Use generative model + search algorithm to propose a small batch of high-scoring candidates and query those candidates with the ground truth oracle.
- (4) Add the newly-obtained (input, label) pairs to the dataset.
- (5) Retrain the generative model or fine-tune.
- (6) Repeat steps (3)-(5).

For (3), examples of "use generative model" may include:
- Using the generative model as a prior, e.g. if $\pt(x,y)=\pt(x|y)p(y)$, then the search algorithm can initialise its starting point via a sample from $\pt(x|y)$.
- The search algorithm evaluates the density of an input $\pt(x)$, which is possible with certain models such as diffusion and normalising flows.

Here is a more pragmatic and concrete sketch of this algorithm. To avoid any bias due to optimism, we use the validation surrogate $\fvalid$ as the ground truth, and save the final evaluation with $f$ until the very end.

- (1) Assume offline dataset $\mathcal{D}$, split into $\dtrain$, $\dvalid$, and $\dtest$.
- (2) Given: $\dtrain$, $\dvalid$, $\ftrain$ trained on $\dtrain$, $\fvalid$ trained on $\dtrain \cup \dvalid$
- (3) Train $\pt$ on $\dtrain$, use $\dvalid$ for model selection.
- (4) Pretend we're in online mode now. For $t = 1, \dots, T$:
  - (4a) (Search algorithm) Use $\pt$ and $\ftrain$ to search for high-scoring candidates.
  - (4b) Score candidates with $\fvalid$, compute mean reward $r_t$, save this value.
  - (4c) Update $\ftrain$ with $\tilde{\mathcal{D}}$ with previous evaluations.
- (5) Compute /discounted/ sum of reward: $G_T = \sum_{t=1}^{T} \gamma^{t-1}r_t$ for discount rate $\gamma$.

The discounted sum is meant to encode the notion that rewards obtained earlier are better than later, as each evaluation increases the /cumulative cost/ from all of the past evaluations performed on the ground truth. In other words, we want to find search algorithms / models which produce good samples as early as possible, as this is when the fewest number of evaluations are performed.

(Hats off to my co-author Alex, who really instilled a sense of MBO needing to be cost-effective. I think this algorithm really hits at the heart of that.)

** open source

Here are some things you may find useful:
- üõ†Ô∏è [[https://github.com/christopher-beckham/validation-metrics-offline-mbo][[validation-metrics-offline-mbo]‚Äã]]: the original code for my paper. This uses the DDPM style of diffusion model from Ho et al.
- üõ†Ô∏è [[https://github.com/christopher-beckham/offline-mbo-edm][[offline-mbo-edm]‚Äã]]: this is a bit more minimalistic and has a more up-to-date diffusion model which is EDM. Not only is this more performant, it generalises existing diffusion models which grants a lot of flexibility when it comes to deciding how to sample.

Also worth noting -- Design Bench can also be a pain to setup, so whichever repo you look at I highly recommend you consult the installation readme [[https://github.com/christopher-beckham/offline-mbo-edm/blob/master/INSTALL.org][here]]. As of time of writing, the mainline branch for Design Bench has broken urls for its datasets, so you should switch to my branch:

#+BEGIN_SRC bash
git clone https://github.com/brandontrabucco/design-bench
git checkout chris/fixes-v2
cd design-bench
pip install . -e
#+END_SRC


* References

#+print_bibliography:
